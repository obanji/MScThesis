{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mimid :  Inferring Grammars from X86 Binaries\n",
    "\n",
    "* Code for subjects [here](#Our-subject-programs)\n",
    "* Evaluation starts [here](#Evaluation)\n",
    "  * The evaluation on specific subjects starts [here](#Subjects)\n",
    "    * [Calculator](#Calculator)\n",
    "    * [Json](#Parson)\n",
    "    * [MJS](#MJS)\n",
    "    * [CSV](#CSV)\n",
    "    * [TinyC](#TinyC)\n",
    "    * [Mini-Lisp](#Komplott)\n",
    "    * [Yxml](#Yxml)\n",
    "    * [Duktape](#Duktape)\n",
    "* Results are [here](#Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:10.734048Z",
     "start_time": "2021-01-03T01:14:10.730316Z"
    }
   },
   "outputs": [],
   "source": [
    "import fuzzingbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X86 instruction as an object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:11.855213Z",
     "start_time": "2021-01-03T01:14:11.851860Z"
    }
   },
   "outputs": [],
   "source": [
    "class Instruction:\n",
    "    def __init__(self, instr):\n",
    "        self.symbol_name = None\n",
    "        self.pointed_address = None\n",
    "        self.dest_reg = None\n",
    "        self.instr_type = None\n",
    "        self._parse(instr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:11.866224Z",
     "start_time": "2021-01-03T01:14:11.859978Z"
    }
   },
   "outputs": [],
   "source": [
    "class Instruction(Instruction):\n",
    "    def get_pointed_value(self, val):\n",
    "        val = val.strip('%*')\n",
    "        if val in REGISTERS:\n",
    "            ptr_addr = gdb.execute('x/s $%s' % (val),\n",
    "                                to_string=True).split(':')\n",
    "            return ptr_addr[0]\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:11.880757Z",
     "start_time": "2021-01-03T01:14:11.869574Z"
    }
   },
   "outputs": [],
   "source": [
    "class Instruction(Instruction):\n",
    "    def resolve_addressing_mode(self, instr):\n",
    "        str0 = instr.split(',')\n",
    "        if len(str0) > 2:\n",
    "            pass\n",
    "\n",
    "        src = str0[-1]\n",
    "        if '(' not in src:\n",
    "            if src[1:] in REGISTERS:\n",
    "\n",
    "                return '$%s' % src[1:]\n",
    "        else:\n",
    "            if src.startswith('-'):\n",
    "                displacement, rest = tuple(src.split('(%'))\n",
    "                return '$%s%s' % (rest[:-1], displacement)\n",
    "            elif src.startswith('(') and src.endswith(')'):\n",
    "                return '$%s' % src[2: -1]\n",
    "            else:\n",
    "                displacement, rest = tuple(src.split('(%'))\n",
    "                return '$%s+%s' % (rest[:-1], displacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:11.891078Z",
     "start_time": "2021-01-03T01:14:11.884716Z"
    }
   },
   "outputs": [],
   "source": [
    "class Instruction(Instruction):\n",
    "    def _parse(self, instr):\n",
    "        instr_list = instr.split()\n",
    "        instr_list.pop(0)\n",
    "\n",
    "        self.current_address = instr_list[0]\n",
    "        if \"<\" in instr_list[1]:\n",
    "            instr_list.pop(1)\n",
    "        self.instr_type = instr_list[1]\n",
    "\n",
    "        if self.instr_type == CALL:\n",
    "            self.pointed_address = self.get_pointed_value(instr_list[2])\n",
    "            if len(instr_list) > 3:\n",
    "                self.symbol_name = instr_list[-1]\n",
    "\n",
    "        elif self.instr_type.startswith('mov') or self.instr_type == 'push' or \\\n",
    "            self.instr_type == 'pop':\n",
    "            self.dest_reg = self.resolve_addressing_mode(instr_list[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting function names from binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:12.393213Z",
     "start_time": "2021-01-03T01:14:12.390731Z"
    }
   },
   "outputs": [],
   "source": [
    "INP_ARR = []\n",
    "VAL_TUPLE = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:12.403789Z",
     "start_time": "2021-01-03T01:14:12.398902Z"
    }
   },
   "outputs": [],
   "source": [
    "def reset_helper():\n",
    "    global INP_ARR \n",
    "    global VAL_TUPLE\n",
    "    INP_ARR.clear()\n",
    "    VAL_TUPLE.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:12.917421Z",
     "start_time": "2021-01-03T01:14:12.911642Z"
    }
   },
   "outputs": [],
   "source": [
    "class BinaryDebugger:\n",
    "    def __init__(self, inp, binary, fn_list):\n",
    "        self.inp = inp\n",
    "        self.binary = binary\n",
    "        self.functions = fn_list\n",
    "        self._set_logger()\n",
    "        self.tree = {}\n",
    "        self.mid = None\n",
    "        self.method_map, self.m_stack = {}, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:12.930128Z",
     "start_time": "2021-01-03T01:14:12.920932Z"
    }
   },
   "outputs": [],
   "source": [
    "class BinaryDebugger(BinaryDebugger):\n",
    "    def break_at(self, address):\n",
    "        gdb.execute(\"break *%s\" % address)\n",
    "    def finish(self):\n",
    "        gdb.execute('finish')\n",
    "    def get_instruction(self):\n",
    "        return gdb.execute('x/i $rip', to_string=True)\n",
    "    def nexti(self):\n",
    "        gdb.execute('nexti')\n",
    "    def resume(self):\n",
    "        gdb.execute('continue')\n",
    "    def run(self):\n",
    "        gdb.execute('run')\n",
    "    def step(self):\n",
    "        gdb.execute('stepi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:12.937725Z",
     "start_time": "2021-01-03T01:14:12.933110Z"
    }
   },
   "outputs": [],
   "source": [
    "class BinaryDebugger(BinaryDebugger):\n",
    "    def start_program(self):\n",
    "        gdb.execute(\"set args '%s'\" % self.inp)\n",
    "        gdb.execute(\"file %s\" % self.binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:12.948191Z",
     "start_time": "2021-01-03T01:14:12.940166Z"
    }
   },
   "outputs": [],
   "source": [
    "class BinaryDebugger(BinaryDebugger):\n",
    "    def _in_scope(self, instr, addr_range):\n",
    "        s1, e1, s2, e2 = addr_range\n",
    "        instr = instr.split()\n",
    "        instr.pop(0)\n",
    "        \n",
    "        current_addr = instr[0].strip(':')\n",
    "        hex_val = int(current_addr, 16)\n",
    "        if hex_val in range(int(s1, 16), int(e1, 16)) or \\\n",
    "            hex_val in range(int(s2, 16), int(e2, 16)):\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:12.957787Z",
     "start_time": "2021-01-03T01:14:12.951283Z"
    }
   },
   "outputs": [],
   "source": [
    "class BinaryDebugger(BinaryDebugger):\n",
    "    def _get_entry_address(self):\n",
    "        self.start_program()\n",
    "        self.run()\n",
    "\n",
    "        info_file = gdb.execute('info file', to_string=True)\n",
    "        entry = None\n",
    "        \n",
    "        for line in info_file.splitlines():\n",
    "            if 'Entry point' in line:\n",
    "                entry = line.split(':')[1]\n",
    "                break\n",
    "        return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:12.966107Z",
     "start_time": "2021-01-03T01:14:12.960659Z"
    }
   },
   "outputs": [],
   "source": [
    "class BinaryDebugger(BinaryDebugger):\n",
    "    def _set_logger(self):\n",
    "        gdb.execute('set logging overwrite on')\n",
    "        gdb.execute('set logging redirect on')\n",
    "        gdb.execute('set logging on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:12.974112Z",
     "start_time": "2021-01-03T01:14:12.968571Z"
    }
   },
   "outputs": [],
   "source": [
    "class BinaryDebugger(BinaryDebugger):\n",
    "    def _get_address_range(self):\n",
    "        s1 = s2 = None\n",
    "        e1 = e2 = None\n",
    "        mappings = gdb.execute('info proc mappings', to_string=True)\n",
    "\n",
    "        for i, line in enumerate(mappings.splitlines()):\n",
    "            if i == 4:\n",
    "                s1 = line.split()[0]\n",
    "            elif i == 6:\n",
    "                e1 = line.split()[1]\n",
    "            elif i == 7:\n",
    "                s2 = line.split()[0]\n",
    "            elif i == 10:\n",
    "                e2 = line.split()[1]\n",
    "        return (s1, e1, s2, e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:12.984615Z",
     "start_time": "2021-01-03T01:14:12.976564Z"
    }
   },
   "outputs": [],
   "source": [
    "class BinaryDebugger(BinaryDebugger):\n",
    "    def _get_main_address(self):\n",
    "        entry = self._get_entry_address()\n",
    "        self.break_at(entry)\n",
    "        gdb.execute('run')\n",
    "\n",
    "        instr = []\n",
    "        while True:\n",
    "            next_i = self.get_instruction()\n",
    "            if CALL in next_i:\n",
    "                break\n",
    "            instr.append(next_i)\n",
    "            self.step()\n",
    "\n",
    "        instr = instr[-1].split()\n",
    "        if len(instr) == 6:\n",
    "            s = instr[3]\n",
    "        else:\n",
    "            s = instr[4]\n",
    "\n",
    "        reg = s[-3:]\n",
    "        main_addr = gdb.execute('p/x $%s' % reg, to_string=True)\n",
    "        main_addr = main_addr.partition(\"= \")\n",
    "        main_addr = main_addr[-1]\n",
    "\n",
    "        return main_addr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:12.991335Z",
     "start_time": "2021-01-03T01:14:12.986876Z"
    }
   },
   "outputs": [],
   "source": [
    "class BinaryDebugger(BinaryDebugger):\n",
    "    def _lookup_address(self, addr, symbol):\n",
    "        addr = addr.rstrip(\"\\n\")\n",
    "        if addr in self.functions.keys():\n",
    "            return self.functions[addr]\n",
    "        else:\n",
    "            if symbol:\n",
    "                s0 = symbol[1:-1].split('@')[0]\n",
    "                return s0\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:12.997203Z",
     "start_time": "2021-01-03T01:14:12.993567Z"
    }
   },
   "outputs": [],
   "source": [
    "class BinaryDebugger(BinaryDebugger):\n",
    "    def _init_methodMap_mtdStack(self, mname):\n",
    "        self.method_map = {'0':[0, None, [1]], '1': [1, mname, []]}\n",
    "        self.m_stack = ['0', '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:13.005147Z",
     "start_time": "2021-01-03T01:14:13.000903Z"
    }
   },
   "outputs": [],
   "source": [
    "class BinaryDebugger(BinaryDebugger):\n",
    "    def _init_result(self, inp, arg1):\n",
    "        self.result = {'inputstr': inp,\n",
    "                'arg': inp,\n",
    "                'original': arg1,\n",
    "                'comparisons': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:13.020162Z",
     "start_time": "2021-01-03T01:14:13.007015Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class BinaryDebugger(BinaryDebugger):\n",
    "    def event_loop(self):\n",
    "        main = self._get_main_address()\n",
    "        mname = self._lookup_address(main, None)\n",
    "        cs = CallStack()\n",
    "        cs.enter(mname)\n",
    "\n",
    "        self._init_methodMap_mtdStack(mname)     \n",
    "        self._init_result(self.inp, arg1)  \n",
    "        self.break_at(main)\n",
    "        self.resume()\n",
    "        addr_range = self._get_address_range()\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                nexti = self.get_instruction()\n",
    "                if self._in_scope(nexti, addr_range):\n",
    "                    h = Instruction(nexti)\n",
    "                    if h.instr_type == CALL:\n",
    "                        name = self._lookup_address(h.pointed_address, h.symbol_name)\n",
    "                        if not name:\n",
    "                            self.step()\n",
    "                            self.finish()\n",
    "                        else:\n",
    "                            self.step()\n",
    "                            cs.enter(name)\n",
    "                            x, self.mid = cs.method_id\n",
    "                            self.method_map[self.m_stack[-1]][-1].append(self.mid)\n",
    "                            self.method_map[str(self.mid)] = [self.mid, name, []]\n",
    "                            self.m_stack.append(str(self.mid))\n",
    "                    elif h.instr_type == RETURN:\n",
    "                        self.step()\n",
    "                        cs.leave()\n",
    "                        if len(self.m_stack) > 1: self.m_stack.pop()\n",
    "                    else:\n",
    "                        self.step()\n",
    "                        val = read_register_val(h.dest_reg, self.inp)\n",
    "                        comparison = process_value(val, self.mid, self.inp)\n",
    "                        if comparison != None:\n",
    "                            self.result['comparisons'].append(comparison)\n",
    "                else:\n",
    "                    self.finish()\n",
    "            except gdb.error:\n",
    "                break\n",
    "        self.result['method_map'] = self.method_map\n",
    "        with open('tree', 'w+') as f:\n",
    "            obj = jsonpickle.encode(self.result)\n",
    "            f.write(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:13.074176Z",
     "start_time": "2021-01-03T01:14:13.022732Z"
    }
   },
   "outputs": [],
   "source": [
    "from fuzzingbook import fuzzingbook_utils\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:13.098138Z",
     "start_time": "2021-01-03T01:14:13.076097Z"
    }
   },
   "outputs": [],
   "source": [
    "head = \"\"\"\\\n",
    "import sys\n",
    "sys.path.extend([%s])\n",
    "sys.path.append('.')\n",
    "import matplotlib.pyplot\n",
    "matplotlib.pyplot._IP_REGISTERED = True # Hack\n",
    "#import fuzzingbook_utils\n",
    "import fuzzingbook\n",
    "from fuzzingbook.GrammarMiner import CallStack\n",
    "import jsonpickle\n",
    "import os, subprocess\n",
    "import gdb\n",
    "import re, json\n",
    "\"\"\" % (', '.join(\"'%s'\" % str(i) for i in sys.path if i))\n",
    "Instruction_src = fuzzingbook_utils.extract_class_definition(Instruction)\n",
    "BinaryDebugger_src = fuzzingbook_utils.extract_class_definition(BinaryDebugger)\n",
    "\n",
    "declaration = \"\"\"\n",
    "CALL = 'callq'\n",
    "RETURN = 'retq'\n",
    "LINE = 'line'\n",
    "ARG_REGISTERS = ['rdi', 'rsi', 'rdx', 'rcx', 'r8', 'r9', 'edi', 'rbx']\n",
    "REGISTERS = ARG_REGISTERS + ['rax', 'eax', 'edi' 'esi', 'edx', 'ecx', 'rsp', 'rbp']\n",
    "\"\"\"\n",
    "helper1 = \"\"\"\n",
    "INP_ARR = []\n",
    "VAL_TUPLE = []\n",
    "\n",
    "def reset_helper():\n",
    "    global INP_ARR \n",
    "    global VAL_TUPLE\n",
    "    INP_ARR.clear()\n",
    "    VAL_TUPLE.clear()\n",
    "    \n",
    "def get_names_from_symbols(objfile):\n",
    "    names = []\n",
    "    for name in objfile:\n",
    "        name = name.split()\n",
    "        name = name[-1].decode('utf-8')\n",
    "        if '@@' in name:\n",
    "            names.append(name.split('@@')[0])\n",
    "            continue\n",
    "        names.append(name)\n",
    "    return names\n",
    "\n",
    "def list_objfile_symbols():\n",
    "    proc = subprocess.Popen(['nm', 'a.out'], stdout=subprocess.PIPE)\n",
    "    output = proc.stdout.read()\n",
    "    output = output.splitlines()\n",
    "    return output\n",
    "\n",
    "def get_function_names(inp, binary):\n",
    "    fn_dict = {}\n",
    "    fn_names = []\n",
    "\n",
    "    symbols = list_objfile_symbols()\n",
    "    functions = get_names_from_symbols(symbols)\n",
    "\n",
    "    gdb.execute(\"set args '%s'\" % inp)\n",
    "    gdb.execute(\"file %s\" % binary)\n",
    "    gdb.execute('set confirm off')\n",
    "    gdb.execute('run')\n",
    "    for k in functions:\n",
    "        try:\n",
    "            s = gdb.execute('info address %s' % k,\n",
    "            to_string=True).split(' ')\n",
    "            if s[4].startswith('0x'):\n",
    "                v = s[4].rstrip()\n",
    "                u = v.strip('.')\n",
    "                fn_dict[v] = k\n",
    "            else:\n",
    "                u = s[-1].rstrip()\n",
    "                u = u.strip('.')\n",
    "                fn_dict[u] = k\n",
    "        except gdb.error:\n",
    "            continue\n",
    "    return fn_dict\n",
    "\"\"\"\n",
    "helper2 = \"\"\"\n",
    "def read_register_val(reg, original):\n",
    "    if not reg:\n",
    "        return None\n",
    "\n",
    "    val = read_as_string(reg)\n",
    "    if not val:\n",
    "        return None\n",
    "    if val == '\"\"':\n",
    "        a = read_ptr_addr(reg)\n",
    "        return read_register_val(a, original)\n",
    "    elif val in original:\n",
    "        return val\n",
    "    else:\n",
    "        x = val[1: -1] if val[0] == '\"' and val[-1] == '\"' else val\n",
    "        return x if x in original else None\n",
    "\n",
    "def read_ptr_addr(reg):\n",
    "    try:\n",
    "        str1 = gdb.execute('x/a %s' % (reg), to_string=True)\n",
    "        for idx, char in enumerate(str1):\n",
    "            if str1[idx] == ':':\n",
    "                addr_val = str1[idx + 1:]\n",
    "                addr_val = addr_val.strip()\n",
    "                break\n",
    "        return addr_val\n",
    "    except Exception:\n",
    "        return\n",
    "        \n",
    "def read_as_string(reg):\n",
    "    try:\n",
    "        str0 = gdb.execute('x/s %s' % (reg), to_string=True)\n",
    "        if '<error:' in str0 and not reg.startswith('0x'):\n",
    "            x = gdb.execute('p/c %s' % reg, to_string=True)\n",
    "            x = x.split()\n",
    "            x = x[-1]\n",
    "            return x[1:-1]\n",
    "\n",
    "        for idx, char in enumerate(str0):\n",
    "            if str0[idx] == ':':\n",
    "                str_val = str0[idx + 1:]\n",
    "                str_val = str_val.strip()\n",
    "                break\n",
    "        return str_val\n",
    "    except Exception:\n",
    "        return\n",
    "\n",
    "def process_value(val, mid, inputstr):\n",
    "    if val and len(val) == 1:\n",
    "        INP_ARR.append(val)\n",
    "        x = ''.join(INP_ARR)\n",
    "\n",
    "        if inputstr.startswith(x) and mid not in VAL_TUPLE:\n",
    "            VAL_TUPLE.append(mid)\n",
    "            idx = len(INP_ARR) - 1\n",
    "            return [idx, val, mid]\n",
    "        else:\n",
    "            INP_ARR.pop()\n",
    "\"\"\"\n",
    "tail = \"\"\"\n",
    "reset_helper()\n",
    "arg_0 = None\n",
    "with open(f'inp.0.txt', 'r+') as f:\n",
    "    arg_0 = f.read().strip()\n",
    "\n",
    "fnames = get_function_names(arg_0, \"a.out\")\n",
    "subprocess.call(['strip', '-s', \"a.out\"])\n",
    "\n",
    "debugger = BinaryDebugger(arg_0, 'a.out', fnames)\n",
    "debugger.event_loop()\n",
    "\"\"\"\n",
    "miner_src = '\\n'.join([head, declaration, Instruction_src, helper1, BinaryDebugger_src, helper2, tail])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:13.106730Z",
     "start_time": "2021-01-03T01:14:13.099908Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('bminer.py', 'w+') as f:\n",
    "    print(miner_src, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstructing the Method Tree with Attached Character Comparisons\n",
    "\n",
    "Reconstruct the actual method trace from a trace with the following\n",
    "format\n",
    "```\n",
    "key   : [ mid, method_name, children_ids ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:13.611383Z",
     "start_time": "2021-01-03T01:14:13.600918Z"
    }
   },
   "outputs": [],
   "source": [
    "def reconstruct_method_tree(method_map):\n",
    "    first_id = None\n",
    "    tree_map = {}\n",
    "    for key in method_map:\n",
    "        m_id, m_name, m_children = method_map[key]\n",
    "        children = []\n",
    "        if m_id in tree_map:\n",
    "            # just update the name and children\n",
    "            assert not tree_map[m_id]\n",
    "            tree_map[m_id]['id'] = m_id\n",
    "            tree_map[m_id]['name'] = m_name\n",
    "            tree_map[m_id]['indexes'] = []\n",
    "            tree_map[m_id]['children'] = children\n",
    "        else:\n",
    "            assert first_id is None\n",
    "            tree_map[m_id] = {'id': m_id, 'name': m_name, 'children': children, 'indexes': []}\n",
    "            first_id = m_id\n",
    "\n",
    "        for c in m_children:\n",
    "            assert c not in tree_map\n",
    "            val = {}\n",
    "            tree_map[c] = val\n",
    "            children.append(val)\n",
    "    return first_id, tree_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:13.629565Z",
     "start_time": "2021-01-03T01:14:13.613525Z"
    }
   },
   "outputs": [],
   "source": [
    "from fuzzingbook.GrammarFuzzer import display_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying last comparisons\n",
    "We need only the last comparisons made on any index. This means that we should care for only the last parse in an ambiguous parse. So, we assign the method that last touched an index to be its consumer.\n",
    "\n",
    "However, to make concessions for real world, we also check if we are overwriting a child (`HEURISTIC`). Essentially, if the heursitic is enabled, then if the current method id (`midP`) is smaller than the `midC` already stored in the last comparison map, then it means that `midP` is a parent that called `midC` previously, and now accessing an index that `midC` touched. This happens when the parent tries to find a substring like `#` in the entirety of the original string. (Note that we have seen this only in `URLParser`). (Note that this heuristic does not restrict reparsing by another function call -- in such a case, `midC` will not smaller than `midP`). So, perhaps, we should let the child keep the ownership. However, there is one more wrinkle. If the character being contested was the last index touched by our `mid`, then it is likely that it was simply a boundary check. In that case, we should let the parent own this character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:14.139131Z",
     "start_time": "2021-01-03T01:14:14.136930Z"
    }
   },
   "outputs": [],
   "source": [
    "LAST_COMPARISON_HEURISTIC = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:14.149092Z",
     "start_time": "2021-01-03T01:14:14.144225Z"
    }
   },
   "outputs": [],
   "source": [
    "def last_comparisons(comparisons):\n",
    "    last_cmp_only = {}\n",
    "    last_idx = {}\n",
    "\n",
    "    # get the last indexes compared in methods.\n",
    "    for idx, char, mid in comparisons:\n",
    "        if mid in last_idx:\n",
    "            if idx > last_idx[mid]:\n",
    "                last_idx[mid] = idx\n",
    "        else:\n",
    "            last_idx[mid] = idx\n",
    "\n",
    "    for idx, char, mid in comparisons:\n",
    "        if LAST_COMPARISON_HEURISTIC:\n",
    "            if idx in last_cmp_only:\n",
    "                midC = last_cmp_only[idx]\n",
    "                if midC > mid:\n",
    "                    # midC is a child of mid.\n",
    "                    # do not clobber children unless it was the last character\n",
    "                    # for that child.\n",
    "                    if last_idx[mid] == idx:\n",
    "                        # if it was the last index, may be the child used it\n",
    "                        # as a boundary check.\n",
    "                        pass\n",
    "                    else:\n",
    "                        # do not overwrite the current value of `last_cmp_only[idx]`\n",
    "                        continue\n",
    "        last_cmp_only[idx] = mid\n",
    "    return last_cmp_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attaching characters to the tree\n",
    "Add the comparison indexes to the method tree that we constructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:14.696703Z",
     "start_time": "2021-01-03T01:14:14.692410Z"
    }
   },
   "outputs": [],
   "source": [
    "def attach_comparisons(method_tree, comparisons):\n",
    "    for idx in comparisons:\n",
    "        mid = comparisons[idx]\n",
    "        method_tree[mid]['indexes'].append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert a list of indexes to a corresponding terminal tree node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:15.220044Z",
     "start_time": "2021-01-03T01:14:15.216187Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_node(idxes, my_str):\n",
    "    assert len(idxes) == idxes[-1] - idxes[0] + 1\n",
    "    assert min(idxes) == idxes[0]\n",
    "    assert max(idxes) == idxes[-1]\n",
    "    return my_str[idxes[0]:idxes[-1] + 1], [], idxes[0], idxes[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:15.229911Z",
     "start_time": "2021-01-03T01:14:15.227303Z"
    }
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to identify the terminal (leaf) nodes. For that, we want to group contiguous letters in a node together, and call it a leaf node. So, convert our list of indexes to lists of contiguous indexes first, then convert them to terminal tree nodes. Then, return a set of one level child nodes with contiguous chars from indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:15.760494Z",
     "start_time": "2021-01-03T01:14:15.757230Z"
    }
   },
   "outputs": [],
   "source": [
    "def indexes_to_children(indexes, my_str):\n",
    "    lst = [\n",
    "        list(map(itemgetter(1), g))\n",
    "        for k, g in it.groupby(enumerate(indexes), lambda x: x[0] - x[1])\n",
    "    ]\n",
    "\n",
    "    return [to_node(n, my_str) for n in lst]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to remove the overlap from the trees we have so far. The idea is that, given a node, each child node of that node should be uniquely responsible for a specified range of characters, with no overlap allowed between the children. The starting of the first child to ending of the last child will be the range of the node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Overlap\n",
    "If overlap is found, the tie is biased to the later child. That is, the later child gets to keep the range, and the former child is recursively traversed to remove overlaps from its children. If a child is completely included in the overlap, the child is excised. A few convenience functions first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:16.870453Z",
     "start_time": "2021-01-03T01:14:16.866639Z"
    }
   },
   "outputs": [],
   "source": [
    "def does_item_overlap(r, r_):\n",
    "    (s, e), (s_, e_) = r, r_\n",
    "    return ((s_ >= s and s_ <= e) or \n",
    "            (e_ >= s and e_ <= e) or \n",
    "            (s_ <= s and e_ >= e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:16.878127Z",
     "start_time": "2021-01-03T01:14:16.875125Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_second_item_included(r, r_):\n",
    "    (s, e), (s_, e_) = r, r_\n",
    "    return (s_ >= s and e_ <= e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:16.887133Z",
     "start_time": "2021-01-03T01:14:16.884163Z"
    }
   },
   "outputs": [],
   "source": [
    "def has_overlap(ranges, r_):\n",
    "    return {r for r in ranges if does_item_overlap(r, r_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:16.896175Z",
     "start_time": "2021-01-03T01:14:16.892336Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_included(ranges, r_):\n",
    "    return {r for r in ranges if is_second_item_included(r, r_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:16.908276Z",
     "start_time": "2021-01-03T01:14:16.902205Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_overlap_from(original_node, orange):\n",
    "    node, children, start, end = original_node\n",
    "    new_children = []\n",
    "    if not children:\n",
    "        return None\n",
    "    start = -1\n",
    "    end = -1\n",
    "    for child in children:\n",
    "        if does_item_overlap(child[2:4], orange):\n",
    "            new_child = remove_overlap_from(child, orange)\n",
    "            if new_child: # and new_child[1]:\n",
    "                if start == -1: start = new_child[2]\n",
    "                new_children.append(new_child)\n",
    "                end = new_child[3]\n",
    "        else:\n",
    "            new_children.append(child)\n",
    "            if start == -1: start = child[2]\n",
    "            end = child[3]\n",
    "    if not new_children:\n",
    "        return None\n",
    "    assert start != -1\n",
    "    assert end != -1\n",
    "    return (node, new_children, start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that there is no overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:17.454271Z",
     "start_time": "2021-01-03T01:14:17.448327Z"
    }
   },
   "outputs": [],
   "source": [
    "def no_overlap(arr):\n",
    "    my_ranges = {}\n",
    "    for a in arr:\n",
    "        _, _, s, e = a\n",
    "        r = (s, e)\n",
    "        included = is_included(my_ranges, r)\n",
    "        if included:\n",
    "            continue  # we will fill up the blanks later.\n",
    "        else:\n",
    "            overlaps = has_overlap(my_ranges, r) \n",
    "            if overlaps:\n",
    "                # unlike include which can happen only once in a set of\n",
    "                # non-overlapping ranges, overlaps can happen on multiple parts.\n",
    "                # The rule is, the later child gets the say. So, we recursively\n",
    "                # remove any ranges that overlap with the current one from the\n",
    "                # overlapped range.\n",
    "                assert len(overlaps) == 1\n",
    "                oitem = list(overlaps)[0]\n",
    "                v = remove_overlap_from(my_ranges[oitem], r)\n",
    "                del my_ranges[oitem]\n",
    "                if v:\n",
    "                    my_ranges[v[2:4]] = v\n",
    "                my_ranges[r] = a\n",
    "            else:\n",
    "                my_ranges[r] = a\n",
    "    res = my_ranges.values()\n",
    "    # assert no overlap, and order by starting index\n",
    "    s = sorted(res, key=lambda x: x[2])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate derivation tree\n",
    "\n",
    "Convert a mapped tree to the _fuzzingbook_ style derivation tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:17.997819Z",
     "start_time": "2021-01-03T01:14:17.991716Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_tree(node, my_str):\n",
    "    method_name = (\"<%s>\" % node['name']) if node['name'] is not None else '<START>'\n",
    "    indexes = node['indexes']\n",
    "    node_children = [to_tree(c, my_str) for c in node.get('children', [])]\n",
    "    idx_children = indexes_to_children(indexes, my_str)\n",
    "    children = no_overlap([c for c in node_children if c is not None] + idx_children)\n",
    "    if not children:\n",
    "        return None\n",
    "    start_idx = children[0][2]\n",
    "    end_idx = children[-1][3]\n",
    "    si = start_idx\n",
    "    my_children = []\n",
    "    # FILL IN chars that we did not compare. This is likely due to an i + n\n",
    "    # instruction.\n",
    "    for c in children:\n",
    "        if c[2] != si:\n",
    "            sbs = my_str[si: c[2]]\n",
    "            my_children.append((sbs, [], si, c[2] - 1))\n",
    "        my_children.append(c)\n",
    "        si = c[3] + 1\n",
    "\n",
    "    m = (method_name, my_children, start_idx, end_idx)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:18.006287Z",
     "start_time": "2021-01-03T01:14:18.002574Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:18.018922Z",
     "start_time": "2021-01-03T01:14:18.014034Z"
    }
   },
   "outputs": [],
   "source": [
    "def zoom(v, zoom=True):\n",
    "    # return v directly if you do not want to zoom out.\n",
    "    if zoom:\n",
    "        return Image(v.render(format='png'))\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Grammar\n",
    "\n",
    "Generating a grammar from the generalized derivation trees is pretty simple. Start at the start node, and any node that represents a method or a pseudo method becomes a nonterminal. The children forms alternate expansions for the nonterminal. Since all the keys are compatible, merging the grammar is simply merging the hash map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a pretty printer for grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:19.070411Z",
     "start_time": "2021-01-03T01:14:19.067445Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "RE_NONTERMINAL = re.compile(r'(<[^<> ]*>)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:19.081670Z",
     "start_time": "2021-01-03T01:14:19.074569Z"
    }
   },
   "outputs": [],
   "source": [
    "def recurse_grammar(grammar, key, order, canonical):\n",
    "    rules = sorted(grammar[key])\n",
    "    old_len = len(order)\n",
    "    for rule in rules:\n",
    "        if not canonical:\n",
    "            res =  re.findall(RE_NONTERMINAL, rule)\n",
    "        else:\n",
    "            res = rule\n",
    "        for token in res:\n",
    "            if token.startswith('<') and token.endswith('>'):\n",
    "                if token not in order:\n",
    "                    order.append(token)\n",
    "    new = order[old_len:]\n",
    "    for ckey in new:\n",
    "        recurse_grammar(grammar, ckey, order, canonical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:19.089107Z",
     "start_time": "2021-01-03T01:14:19.084356Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_grammar(grammar, start_symbol='<START>', canonical=True):\n",
    "    order = [start_symbol]\n",
    "    recurse_grammar(grammar, start_symbol, order, canonical)\n",
    "    if len(order) != len(grammar.keys()):\n",
    "        assert len(order) < len(grammar.keys())\n",
    "    return {k: sorted(grammar[k]) for k in order}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trees to grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:19.627165Z",
     "start_time": "2021-01-03T01:14:19.623463Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_grammar(tree, grammar):\n",
    "    node, children, _, _ = tree\n",
    "    if not children: return grammar\n",
    "    tokens = []\n",
    "    if node not in grammar:\n",
    "        grammar[node] = list()\n",
    "    for c in children:\n",
    "        tokens.append(c[0])\n",
    "        to_grammar(c, grammar)\n",
    "    grammar[node].append(tuple(tokens))\n",
    "    return grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:19.638538Z",
     "start_time": "2021-01-03T01:14:19.632679Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_grammar(g1, g2):\n",
    "    all_keys = set(list(g1.keys()) + list(g2.keys()))\n",
    "    merged = {}\n",
    "    for k in all_keys:\n",
    "        alts = set(g1.get(k, []) + g2.get(k, []))\n",
    "        merged[k] = alts\n",
    "    return {k:[l for l in merged[k]] for k in merged}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:19.648578Z",
     "start_time": "2021-01-03T01:14:19.641399Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_grammar(my_trees):\n",
    "    grammar = {}\n",
    "    ret = []\n",
    "    for my_tree in my_trees:\n",
    "        tree = my_tree['tree']\n",
    "        start = tree[0]\n",
    "        src_file = my_tree['original']\n",
    "        arg_file = my_tree['arg']\n",
    "        ret.append((start, src_file, arg_file))\n",
    "        g = to_grammar(tree, grammar)\n",
    "        grammar = merge_grammar(grammar, g)\n",
    "    return grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting Empty Alternatives for IF and Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to insert empty rules for those loops and conditionals that can be skipped. For loops, the entire sequence has to contain the empty marker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:20.742485Z",
     "start_time": "2021-01-03T01:14:20.735692Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_empty_rules(grammar):\n",
    "    new_grammar = {}\n",
    "    for k in grammar:\n",
    "        if k in ':if_':\n",
    "            name, marker = k.split('#')\n",
    "            if name.endswith(' *'):\n",
    "                new_grammar[k] = grammar[k].add(('',))\n",
    "            else:\n",
    "                new_grammar[k] = grammar[k]\n",
    "        elif k in ':while_':\n",
    "            # TODO -- we have to check the rules for sequences of whiles.\n",
    "            # for now, ignore.\n",
    "            new_grammar[k] = grammar[k]\n",
    "        else:\n",
    "            new_grammar[k] = grammar[k]\n",
    "    return new_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to generalize the loops. The idea is to look for patterns exclusively in the similarly named while loops using any of the regular expression learners. For the prototype, we replaced the modified Sequitur with the modified Fernau which gave us better regular expressions than before. The main constraint we have is that we want to avoid repeated execution of program if possible. Fernau algorithm can recover a reasonably approximate regular exression based only on positive data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The modified Fernau algorithm\n",
    "\n",
    "The Fernau algorithm is from _Algorithms for learning regular expressions from positive data_ by _HenningFernau_. Our algorithm uses a modified form of the Prefix-Tree-Acceptor from Fernau. First we define an LRF buffer of a given size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:22.451566Z",
     "start_time": "2021-01-03T01:14:22.447724Z"
    }
   },
   "outputs": [],
   "source": [
    "class Buf:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.items = [None] * self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `add1()` takes in an array, and transfers the first element of the array into the end of current buffer, and simultaneously drops the first element of the buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:22.988812Z",
     "start_time": "2021-01-03T01:14:22.984715Z"
    }
   },
   "outputs": [],
   "source": [
    "class Buf(Buf):\n",
    "    def add1(self, items):\n",
    "        self.items.append(items.pop(0))\n",
    "        return self.items.pop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For equality between the buffer and an array, we only compare when both the array and the items are actually elements and not chunked arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:23.534192Z",
     "start_time": "2021-01-03T01:14:23.529966Z"
    }
   },
   "outputs": [],
   "source": [
    "class Buf(Buf):\n",
    "    def __eq__(self, items):\n",
    "        if any(isinstance(i, dict) for i in self.items): return False\n",
    "        if any(isinstance(i, dict) for i in items): return False\n",
    "        return items == self.items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `detect_chunks()` detects any repeating portions of a list of `n` size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:24.296465Z",
     "start_time": "2021-01-03T01:14:24.292255Z"
    }
   },
   "outputs": [],
   "source": [
    "def detect_chunks(n, lst_):\n",
    "    lst = list(lst_)\n",
    "    chunks = set()\n",
    "    last = Buf(n)\n",
    "    # check if the next_n elements are repeated.\n",
    "    for _ in range(len(lst) - n):\n",
    "        lnext_n = lst[0:n]\n",
    "        if last == lnext_n:\n",
    "            # found a repetition.\n",
    "            chunks.add(tuple(last.items))\n",
    "        else:\n",
    "            pass\n",
    "        last.add1(lst)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have detected plausible repeating sequences, we gather all similar sequences into arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:25.272556Z",
     "start_time": "2021-01-03T01:14:25.265636Z"
    }
   },
   "outputs": [],
   "source": [
    "def chunkify(lst_,n , chunks):\n",
    "    lst = list(lst_)\n",
    "    chunked_lst = []\n",
    "    while len(lst) >= n:\n",
    "        lnext_n = lst[0:n]\n",
    "        if (not any(isinstance(i, dict) for i in lnext_n)) and tuple(lnext_n) in chunks:\n",
    "            chunked_lst.append({'_':lnext_n})\n",
    "            lst = lst[n:]\n",
    "        else:\n",
    "            chunked_lst.append(lst.pop(0))\n",
    "    chunked_lst.extend(lst)\n",
    "    return chunked_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `identify_chunks()` simply calls the `detect_chunks()` on all given lists, and then converts all chunks identified into arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:26.047187Z",
     "start_time": "2021-01-03T01:14:26.041231Z"
    }
   },
   "outputs": [],
   "source": [
    "def identify_chunks(my_lsts):\n",
    "    # initialize\n",
    "    all_chunks = {}\n",
    "    maximum = max(len(lst) for lst in my_lsts)\n",
    "    for i in range(1, maximum//2+1):\n",
    "        all_chunks[i] = set()\n",
    "\n",
    "    # First, identify chunks in each list.\n",
    "    for lst in my_lsts:\n",
    "        for i in range(1,maximum//2+1):\n",
    "            chunks = detect_chunks(i, lst)\n",
    "            all_chunks[i] |= chunks\n",
    "\n",
    "    # Then, chunkify\n",
    "    new_lsts = []\n",
    "    for lst in my_lsts:\n",
    "        for i in range(1,maximum//2+1):\n",
    "            chunks = all_chunks[i]\n",
    "            lst = chunkify(lst, i, chunks)\n",
    "        new_lsts.append(lst)\n",
    "    return new_lsts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prefix tree acceptor\n",
    "\n",
    "The prefix tree acceptor is a way to represent positive data. The `Node` class holds a single node in the prefix tree acceptor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:26.764291Z",
     "start_time": "2021-01-03T01:14:26.754841Z"
    }
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    # Each tree node gets its unique id.\n",
    "    _uid = 0\n",
    "    def __init__(self, item):\n",
    "        # self.repeats = False\n",
    "        self.count = 1 # how many repetitions.\n",
    "        self.counters = set()\n",
    "        self.last = False\n",
    "        self.children = []\n",
    "        self.item = item\n",
    "        self.uid = Node._uid\n",
    "        Node._uid += 1\n",
    "\n",
    "    def update_counters(self):\n",
    "        self.counters.add(self.count)\n",
    "        self.count = 0\n",
    "        for c in self.children:\n",
    "            c.update_counters()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json())\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(\"(%s, [%s])\", (self.item, ' '.join([str(i) for i in self.children])))\n",
    "\n",
    "    def to_json(self):\n",
    "        s = (\"(%s)\" % ' '.join(self.item['_'])) if isinstance(self.item, dict) else str(self.item)\n",
    "        return (s, tuple(self.counters), [i.to_json() for i in self.children])\n",
    "\n",
    "    def inc_count(self):\n",
    "        self.count += 1\n",
    "\n",
    "    def add_ref(self):\n",
    "        self.count = 1\n",
    "\n",
    "    def get_child(self, c):\n",
    "        for i in self.children:\n",
    "            if i.item == c: return i\n",
    "        return None\n",
    "\n",
    "    def add_child(self, c):\n",
    "        # first check if it is the current node. If it is, increment\n",
    "        # count, and return ourselves.\n",
    "        if c == self.item:\n",
    "            self.inc_count()\n",
    "            return self\n",
    "        else:\n",
    "            # check if it is one of the children. If it is a child, then\n",
    "            # preserve its original count.\n",
    "            nc = self.get_child(c)\n",
    "            if nc is None:\n",
    "                nc = Node(c)\n",
    "                self.children.append(nc)\n",
    "            else:\n",
    "                nc.add_ref()\n",
    "            return nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `update_tree()` essentially transforms a list of nodes to a chain of nodes starting at `root` if the `root` is an empty tree. If the `root` already contains a tree, the `update_tree()` traverses the path represented by `lst_` and makes a new child branch where the path specified doesn't exist in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:27.393278Z",
     "start_time": "2021-01-03T01:14:27.388174Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_tree(lst_, root):\n",
    "    lst = list(lst_)\n",
    "    branch = root\n",
    "    while lst:\n",
    "        first, *lst = lst\n",
    "        branch = branch.add_child(first)\n",
    "    branch.last = True\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a number of lists, the `create_tree_with_lists()` creates an actual tree out of these lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:28.096893Z",
     "start_time": "2021-01-03T01:14:28.092015Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_tree_with_lsts(lsts):\n",
    "    Node._uid = 0\n",
    "    root =  Node(None)\n",
    "    for lst in lsts:\n",
    "        root.count = 1 # there is at least one element.\n",
    "        update_tree(lst, root)\n",
    "        root.update_counters()\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a node, and a key, return the key and alts as a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:28.910661Z",
     "start_time": "2021-01-03T01:14:28.901538Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_star(node, key):\n",
    "    if node.item is None:\n",
    "        return [], {}\n",
    "    if isinstance(node.item, dict):\n",
    "        # take care of counters\n",
    "        elements = node.item['_']\n",
    "        my_key = \"<%s-%d-s>\" % (key, node.uid)\n",
    "        alts = [elements]\n",
    "        if len(node.counters) > 1: # repetition\n",
    "            alts.append(elements + [my_key])\n",
    "        return [my_key], {my_key:alts}\n",
    "    else:\n",
    "        return [str(node.item)], {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:28.929525Z",
     "start_time": "2021-01-03T01:14:28.917825Z"
    }
   },
   "outputs": [],
   "source": [
    "def node_to_grammar(node, grammar, key):\n",
    "    rule = []\n",
    "    alts = [rule]\n",
    "    if node.uid == 0:\n",
    "        my_key = \"<%s>\" % key\n",
    "    else:\n",
    "        my_key = \"<%s-%d>\" % (key, node.uid)\n",
    "    grammar[my_key] = alts\n",
    "    if node.item is not None:\n",
    "        mk, g = get_star(node, key)\n",
    "        rule.extend(mk)\n",
    "        grammar.update(g)\n",
    "    # is the node last?\n",
    "    if node.last:\n",
    "        assert node.item is not None\n",
    "        # add a duplicate rule that ends here.\n",
    "        ending_rule = list(rule)\n",
    "        # if there are no children, the current rule is\n",
    "        # any way ending.\n",
    "        if node.children:\n",
    "            alts.append(ending_rule)\n",
    "\n",
    "    if node.children:\n",
    "        if len(node.children) > 1:\n",
    "            my_ckey = \"<%s-%d-c>\" % (key, node.uid)\n",
    "            rule.append(my_ckey)\n",
    "            grammar[my_ckey] = [ [\"<%s-%d>\" % (key, c.uid)] for c in node.children]\n",
    "        else:\n",
    "            my_ckey = \"<%s-%d>\" % (key, node.children[0].uid)\n",
    "            rule.append(my_ckey)\n",
    "    else:\n",
    "        pass\n",
    "    for c in node.children:\n",
    "        node_to_grammar(c, grammar, key)\n",
    "    return grammar\n",
    "\n",
    "def generate_grammar(lists, key):\n",
    "    lsts = identify_chunks(lists)\n",
    "    tree = create_tree_with_lsts(lsts)\n",
    "    grammar = {}\n",
    "    node_to_grammar(tree, grammar, key)\n",
    "    return grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T13:14:08.527784Z",
     "start_time": "2020-11-14T13:14:08.519166Z"
    }
   },
   "source": [
    "Given a rule, determine the abstraction for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:29.673972Z",
     "start_time": "2021-01-03T01:14:29.670705Z"
    }
   },
   "outputs": [],
   "source": [
    "def collapse_alts(rules, k):\n",
    "    ss = [[str(r) for r in rule] for rule in rules]\n",
    "    x = generate_grammar(ss, k[1:-1])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:29.686071Z",
     "start_time": "2021-01-03T01:14:29.678794Z"
    }
   },
   "outputs": [],
   "source": [
    "def collapse_rules(grammar):\n",
    "    r_grammar = {}\n",
    "    for k in grammar:\n",
    "        new_grammar = collapse_alts(grammar[k], k)\n",
    "        # merge the new_grammar with r_grammar\n",
    "        # we know none of the keys exist in r_grammar because\n",
    "        # new keys are k prefixed.\n",
    "        for k_ in new_grammar:\n",
    "            r_grammar[k_] = new_grammar[k_]\n",
    "    return r_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:29.694294Z",
     "start_time": "2021-01-03T01:14:29.688642Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_spaces_in_keys(grammar):\n",
    "    keys = {key: key.replace(' ', '_') for key in grammar}\n",
    "    new_grammar = {}\n",
    "    for key in grammar:\n",
    "        new_alt = []\n",
    "        for rule in grammar[key]:\n",
    "            new_rule = []\n",
    "            for t in rule:\n",
    "                for k in keys:\n",
    "                    t = t.replace(k, keys[k])\n",
    "                new_rule.append(t)\n",
    "            new_alt.append(new_rule)\n",
    "        new_grammar[keys[key]] = new_alt\n",
    "    return new_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicate and redundant entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT** we indicate things that operate on canonical by _c, and those that operate on fuzzable by _f, and both by _cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:31.048338Z",
     "start_time": "2021-01-03T01:14:31.045092Z"
    }
   },
   "outputs": [],
   "source": [
    "def first_in_chain(token, chain):\n",
    "    while True:\n",
    "        if token in chain:\n",
    "            token = chain[token]\n",
    "            assert isinstance(token, str)\n",
    "        else:\n",
    "            break\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return a new symbol for `grammar` based on `symbol_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:31.632510Z",
     "start_time": "2021-01-03T01:14:31.628209Z"
    }
   },
   "outputs": [],
   "source": [
    "def new_symbol(grammar, symbol_name=\"<symbol>\"):\n",
    "    if symbol_name not in grammar:\n",
    "        return symbol_name\n",
    "\n",
    "    count = 1\n",
    "    while True:\n",
    "        tentative_symbol_name = symbol_name[:-1] + \"-\" + repr(count) + \">\"\n",
    "        if tentative_symbol_name not in grammar:\n",
    "            return tentative_symbol_name\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace keys that have a single token definition with the token in the defition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:32.213334Z",
     "start_time": "2021-01-03T01:14:32.207013Z"
    }
   },
   "outputs": [],
   "source": [
    "def replacement_candidate_chains(grammar, ignores):\n",
    "    to_replace = {}\n",
    "    for k in grammar:\n",
    "        if k in ignores: continue\n",
    "        if len(grammar[k]) != 1: continue\n",
    "        rule = grammar[k][0]\n",
    "        if len(rule) != 1: continue\n",
    "        if is_nt(rule[0]):\n",
    "            to_replace[k] = rule[0]\n",
    "        else:\n",
    "            pass\n",
    "    return to_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:32.222867Z",
     "start_time": "2021-01-03T01:14:32.217583Z"
    }
   },
   "outputs": [],
   "source": [
    "def replace_key_by_new_key(grammar, keys_to_replace):\n",
    "    new_grammar = {}\n",
    "    for key in grammar:\n",
    "        new_rules = []\n",
    "        for rule in grammar[key]:\n",
    "            new_rule = [keys_to_replace.get(token, token)\n",
    "                        for token in rule]\n",
    "            new_rules.append(new_rule)\n",
    "        new_grammar[keys_to_replace.get(key, key)] = new_rules\n",
    "    assert len(grammar) == len(new_grammar)\n",
    "    return new_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:32.232846Z",
     "start_time": "2021-01-03T01:14:32.224886Z"
    }
   },
   "outputs": [],
   "source": [
    "def replace_key_by_key(grammar, keys_to_replace):\n",
    "    new_grammar = {}\n",
    "    for key in grammar:\n",
    "        if key in keys_to_replace: continue\n",
    "        new_rules = []\n",
    "        for rule in grammar[key]:\n",
    "            for t in rule:\n",
    "                assert isinstance(t, str)\n",
    "            new_rule = [first_in_chain(token, keys_to_replace) for token in rule]\n",
    "            new_rules.append(new_rule)\n",
    "        new_grammar[key] = new_rules\n",
    "    return new_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:32.239019Z",
     "start_time": "2021-01-03T01:14:32.234959Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_single_entries(grammar):\n",
    "    keys_to_replace = replacement_candidate_chains(grammar, {start_symbol, '<main>'})\n",
    "    return replace_key_by_key(grammar, keys_to_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T13:40:41.420007Z",
     "start_time": "2020-11-14T13:40:41.407390Z"
    }
   },
   "source": [
    "Remove keys that have similar rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:33.088035Z",
     "start_time": "2021-01-03T01:14:33.082813Z"
    }
   },
   "outputs": [],
   "source": [
    "def collect_duplicate_rule_keys(grammar):\n",
    "    collect = {}\n",
    "    for k in grammar:\n",
    "        salt = str(sorted(grammar[k]))\n",
    "        if salt not in collect:\n",
    "            collect[salt] = (k, set())\n",
    "        else:\n",
    "            collect[salt][1].add(k)\n",
    "    return collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:33.102450Z",
     "start_time": "2021-01-03T01:14:33.097185Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_duplicate_rule_keys(grammar):\n",
    "    g = grammar\n",
    "    while True:\n",
    "        collect = collect_duplicate_rule_keys(g)\n",
    "        keys_to_replace = {}\n",
    "        for salt in collect:\n",
    "            k, st = collect[salt]\n",
    "            for s in st:\n",
    "                keys_to_replace[s] = k\n",
    "        if not keys_to_replace:\n",
    "            break\n",
    "        g = replace_key_by_key(g, keys_to_replace)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all the control flow vestiges from names, and simply name them sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:33.898007Z",
     "start_time": "2021-01-03T01:14:33.893408Z"
    }
   },
   "outputs": [],
   "source": [
    "def deep_copy(t): # Python deepcopy is a bit buggy\n",
    "    v = json.dumps(t)\n",
    "    return json.loads(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:33.913278Z",
     "start_time": "2021-01-03T01:14:33.909023Z"
    }
   },
   "outputs": [],
   "source": [
    "def collect_replacement_keys(grammar):\n",
    "    g = deep_copy(grammar)\n",
    "    to_replace = {}\n",
    "    for k in grammar:\n",
    "        if ':' in k:\n",
    "            first, rest = k.split(':')\n",
    "            sym = new_symbol(g, symbol_name=first + '>')\n",
    "            assert sym not in g\n",
    "            g[sym] = None\n",
    "            to_replace[k] = sym\n",
    "        else:\n",
    "            continue\n",
    "    return to_replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove keys that are referred to only from a single rule, and which have a single alternative.\n",
    "Import. This can't work on canonical representation. First, given a key, we figure out its distance to `<START>`.\n",
    "\n",
    "This is different from `remove_single_entries()` in that, there we do not care if the key is being used multiple times. Here, we only replace keys that are referred to only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:34.689206Z",
     "start_time": "2021-01-03T01:14:34.685758Z"
    }
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:34.709159Z",
     "start_time": "2021-01-03T01:14:34.702392Z"
    }
   },
   "outputs": [],
   "source": [
    "def len_to_start(item, parents, start_symbol, seen=None):\n",
    "    if seen is None: seen = set()\n",
    "    if item in seen: return math.inf\n",
    "    seen.add(item)\n",
    "    if item == start_symbol: return 0\n",
    "    else: return 1 + min(len_to_start(p, parents, start_symbol, seen)\n",
    "                         for p in parents[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:34.717607Z",
     "start_time": "2021-01-03T01:14:34.711726Z"
    }
   },
   "outputs": [],
   "source": [
    "def order_by_length_to_start(items, parent_map, start_symbol):\n",
    "    return sorted(items, key=lambda i: len_to_start(i, parent_map, start_symbol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T13:43:26.343274Z",
     "start_time": "2020-11-14T13:43:26.337394Z"
    }
   },
   "source": [
    "Next, we generate a map of `child -> [parents]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:35.659633Z",
     "start_time": "2021-01-03T01:14:35.648805Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_parents_of_tokens(grammar, key, seen=None, parents=None):\n",
    "    if parents is None: parents, seen = {}, set()\n",
    "    if key in seen: return parents\n",
    "    seen.add(key)\n",
    "    for res in grammar[key]:\n",
    "        for token in res:\n",
    "            if not is_nt(token): continue\n",
    "            parents.setdefault(token, []).append(key)\n",
    "    for ckey in {i for i in  grammar if i not in seen}:\n",
    "        get_parents_of_tokens(grammar, ckey, seen, parents)\n",
    "    return parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:35.676773Z",
     "start_time": "2021-01-03T01:14:35.667651Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_references(keys_to_replace):\n",
    "    to_process = list(keys_to_replace.keys())\n",
    "    updated_dict = {}\n",
    "    references = {}\n",
    "    order = []\n",
    "    while to_process:\n",
    "        key, *to_process = to_process\n",
    "        rule = keys_to_replace[key]\n",
    "        new_rule = []\n",
    "        skip = False\n",
    "        for token in rule:\n",
    "            if token not in updated_dict:\n",
    "                if token in to_process:\n",
    "                    # so this token will get defined later. We simply postpone\n",
    "                    # the processing of this key until that key is defined.\n",
    "                    # TODO: check for cycles.\n",
    "                    to_process.append(key)\n",
    "                    references.setdefault(token, set()).add(key)\n",
    "                    skip = True\n",
    "                    break\n",
    "                else:\n",
    "                    new_rule.append(token)\n",
    "            else:\n",
    "                new_rule.extend(updated_dict[token])\n",
    "        if not skip:\n",
    "            order.append(key)\n",
    "            updated_dict[key] = new_rule\n",
    "    return updated_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:35.689225Z",
     "start_time": "2021-01-03T01:14:35.679066Z"
    }
   },
   "outputs": [],
   "source": [
    "def replace_keys_by_rule(grammar, keys_to_replace):\n",
    "    # we now need to verify that none of the keys are part of the sequences.\n",
    "    keys_to_replace = remove_references(keys_to_replace)\n",
    "\n",
    "    new_grammar = {}\n",
    "    for key in grammar:\n",
    "        if key in keys_to_replace: continue\n",
    "\n",
    "        new_rules = []\n",
    "        for rule in grammar[key]:\n",
    "            new_rule = []\n",
    "            for token in rule:\n",
    "                if token in keys_to_replace:\n",
    "                    new_rule.extend(keys_to_replace[token])\n",
    "                else:\n",
    "                    new_rule.append(token)\n",
    "            new_rules.append(new_rule)\n",
    "        new_grammar[key] = new_rules\n",
    "    return new_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:36.433249Z",
     "start_time": "2021-01-03T01:14:36.426939Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_single_alts(grammar, start_symbol):\n",
    "    single_alts = {p for p in grammar if len(grammar[p]) == 1 and p != start_symbol}\n",
    "\n",
    "    child_parent_map = get_parents_of_tokens(grammar, start_symbol)\n",
    "    assert len(child_parent_map) < len(grammar)\n",
    "\n",
    "    single_refs = {p:child_parent_map[p] for p in single_alts if len(child_parent_map[p]) <= 1}\n",
    "\n",
    "    ordered = order_by_length_to_start(single_refs, child_parent_map, start_symbol)\n",
    "\n",
    "    for p in ordered:\n",
    "        assert len(grammar[p]) == 1\n",
    "        if not isinstance(grammar[p][0], str):\n",
    "            print(p, grammar[p][0])\n",
    "\n",
    "    keys_to_replace = {p:grammar[p][0] for p in ordered}\n",
    "    g =  replace_keys_by_rule(grammar, keys_to_replace)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove similar rules from under a single key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:37.262524Z",
     "start_time": "2021-01-03T01:14:37.254894Z"
    }
   },
   "outputs": [],
   "source": [
    "def len_rule(r): return len(r)\n",
    "def len_definition(d): return sum([len_rule(r) for r in d])\n",
    "def len_grammar(g): return sum([len_definition(g[k]) for k in g])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:37.274556Z",
     "start_time": "2021-01-03T01:14:37.270617Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_duplicate_rules_in_a_key(g):\n",
    "    g_ = {}\n",
    "    for k in g:\n",
    "        s = {str(r):r for r in g[k]}\n",
    "        g_[k] = list(sorted(list(s.values())))\n",
    "    return g_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:37.288146Z",
     "start_time": "2021-01-03T01:14:37.277055Z"
    }
   },
   "outputs": [],
   "source": [
    "def grammar_gc(grammar, start_symbol):\n",
    "    def strip_key(grammar, key, order):\n",
    "        rules = sorted(grammar[key])\n",
    "        old_len = len(order)\n",
    "        for rule in rules:\n",
    "            for token in rule:\n",
    "                if is_nt(token):\n",
    "                    if token not in order:\n",
    "                        order.append(token)\n",
    "        new = order[old_len:]\n",
    "        for ckey in new:\n",
    "            strip_key(grammar, ckey, order)\n",
    "\n",
    "    order = [start_symbol]\n",
    "    strip_key(grammar, start_symbol, order)\n",
    "    assert len(order) == len(grammar.keys())\n",
    "    g = {k: sorted(grammar[k]) for k in order}\n",
    "    for k in g:\n",
    "        for r in g[k]:\n",
    "            for t in r:\n",
    "                assert isinstance(t, str)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:37.294843Z",
     "start_time": "2021-01-03T01:14:37.290459Z"
    }
   },
   "outputs": [],
   "source": [
    "def cleanup_grammar(g, start_symbol):\n",
    "    g = grammar_gc(g, start_symbol)\n",
    "    g1 = check_empty_rules(g) # add optional rules\n",
    "    g1 = grammar_gc(g1, start_symbol)\n",
    " \n",
    "    g2 = collapse_rules(g1) # learn regex\n",
    "    g2 = grammar_gc(g2, start_symbol)\n",
    "\n",
    "    g3 = convert_spaces_in_keys(g2) # fuzzable grammar\n",
    "    g3 = grammar_gc(g3, start_symbol)\n",
    "    return g3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:37.307066Z",
     "start_time": "2021-01-03T01:14:37.301232Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_single_entry_chains(grammar, start_symbol):\n",
    "    keys_to_replace = replacement_candidate_chains(grammar, {start_symbol, '<main>'})\n",
    "    return replace_key_by_key(grammar, keys_to_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:37.317773Z",
     "start_time": "2021-01-03T01:14:37.311786Z"
    }
   },
   "outputs": [],
   "source": [
    "def cleanup_token_names(grammar):\n",
    "    keys_to_replace = collect_replacement_keys(grammar)\n",
    "    g = replace_key_by_new_key(grammar, keys_to_replace)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:37.325348Z",
     "start_time": "2021-01-03T01:14:37.320561Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_self_definitions(g):\n",
    "    g_ = {}\n",
    "    for k in g:\n",
    "        rs_ = []\n",
    "        for r in g[k]:\n",
    "            assert not isinstance(r, str)\n",
    "            if len(r) == 1 and r[0] == k: continue\n",
    "            rs_.append(r)\n",
    "        g_[k] = rs_\n",
    "    return g_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:37.342813Z",
     "start_time": "2021-01-03T01:14:37.336542Z"
    }
   },
   "outputs": [],
   "source": [
    "def compact_grammar(e, start_symbol):\n",
    "    assert start_symbol in e\n",
    "    l = len_grammar(e)\n",
    "    diff = 1\n",
    "    while diff > 0:\n",
    "        assert start_symbol in e\n",
    "        e = remove_single_entry_chains(e, start_symbol)\n",
    "        e = grammar_gc(e, start_symbol) # garbage collect\n",
    "\n",
    "        e = remove_duplicate_rule_keys(e)\n",
    "        e = grammar_gc(e, start_symbol) # garbage collect\n",
    "\n",
    "        e = cleanup_token_names(e)\n",
    "        e = grammar_gc(e, start_symbol) # garbage collect\n",
    "\n",
    "        e = remove_single_alts(e, start_symbol)\n",
    "        e = grammar_gc(e, start_symbol) # garbage collect\n",
    "\n",
    "        e = remove_duplicate_rules_in_a_key(e)\n",
    "        e = grammar_gc(e, start_symbol) # garbage collect\n",
    "\n",
    "        e = remove_self_definitions(e)\n",
    "        e = grammar_gc(e, start_symbol) # garbage collect\n",
    "\n",
    "        l_ = len_grammar(e)\n",
    "        diff = l - l_\n",
    "        l = l_\n",
    "    e = grammar_gc(e, start_symbol)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Complete Miner\n",
    "\n",
    "We now put everything together. The `miner()` takes the traces, produces trees out of them, and verifies that the trees actually correspond to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:38.325917Z",
     "start_time": "2021-01-03T01:14:38.320882Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_nt(v):\n",
    "    return len(v) > 1 and (v[0], v[-1]) == ('<', '>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:38.336137Z",
     "start_time": "2021-01-03T01:14:38.330469Z"
    }
   },
   "outputs": [],
   "source": [
    "def tree_to_str(tree): # Non recursive\n",
    "    expanded = []\n",
    "    to_expand = [tree]\n",
    "    while to_expand:\n",
    "        (key, children, *rest), *to_expand = to_expand\n",
    "        if is_nt(key):\n",
    "            to_expand = children + to_expand\n",
    "        else:\n",
    "            assert not children\n",
    "            expanded.append(key)\n",
    "    return ''.join(expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:38.346137Z",
     "start_time": "2021-01-03T01:14:38.340437Z"
    }
   },
   "outputs": [],
   "source": [
    "def miner(call_traces):\n",
    "    my_trees = []\n",
    "    for call_trace in call_traces:\n",
    "        method_map = call_trace['method_map']\n",
    "\n",
    "        first, method_tree = reconstruct_method_tree(method_map)\n",
    "        comparisons = call_trace['comparisons']\n",
    "        attach_comparisons(method_tree, last_comparisons(comparisons))\n",
    "\n",
    "        my_str = call_trace['inputstr']\n",
    "\n",
    "        tree = to_tree(method_tree[first], my_str)\n",
    "        my_tree = {'tree': tree, 'original': call_trace['original'], 'arg': call_trace['arg']}\n",
    "        print(tree_to_str(tree))\n",
    "        assert tree_to_str(tree) == my_str\n",
    "        my_trees.append(my_tree)\n",
    "    return my_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:38.381111Z",
     "start_time": "2021-01-03T01:14:38.348257Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import jsonpickle\n",
    "from fuzzingbook.Parser import non_canonical, canonical\n",
    "def binary_miner(seeds, executable):\n",
    "    call_trace = []\n",
    "    for inp in seeds:\n",
    "        arg0 = '\\'py arg0=\"%s\"\\'' % inp\n",
    "        arg1 = '\\'py arg1=\"%s\"\\'' % executable\n",
    "        \n",
    "        with open(f'inp.0.txt', 'w+') as f:\n",
    "            print(inp, file=f)\n",
    "        \n",
    "        !gcc -g {executable}\n",
    "        !gdb --batch-silent -ex {arg1} -x bminer.py\n",
    "        \n",
    "        with open(f'tree', 'rb') as f:\n",
    "            call_trace.append((jsonpickle.decode(f.read())))\n",
    "            \n",
    "    mined_tree = miner(call_trace)\n",
    "    g0 = convert_to_grammar(mined_tree)\n",
    "    g = cleanup_grammar(g0, start_symbol='<START>')\n",
    "    g = compact_grammar(g, start_symbol='<START>')\n",
    "    return show_grammar(non_canonical(g), canonical=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:39.114101Z",
     "start_time": "2021-01-03T01:14:39.110454Z"
    }
   },
   "outputs": [],
   "source": [
    "Max_Precision = 1000\n",
    "Max_Recall = 1000\n",
    "BMiner = {}\n",
    "BMinerFuzz = {}\n",
    "BMinerGrammar = {}\n",
    "MaxTimeout = 60*60*24 # 2 day\n",
    "MaxParseTimeout = 60*5\n",
    "CHECK = {'calculator', 'parson', 'tinyc', 'csv', 'mjs', 'mini-lisp', 'yxml', 'mini-c'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Recall\n",
    "\n",
    "How many of the *valid* inputs from the golden grammar can be recognized by a parser using our grammar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:39.922726Z",
     "start_time": "2021-01-03T01:14:39.920111Z"
    }
   },
   "outputs": [],
   "source": [
    "from fuzzingbook.Parser import IterativeEarleyParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:39.941577Z",
     "start_time": "2021-01-03T01:14:39.929870Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_recall(golden_grammar, my_grammar, maximum=Max_Recall, start_symbol='<START>', log=False):\n",
    "    my_count = maximum\n",
    "    ie = IterativeEarleyParser(my_grammar, start_symbol=start_symbol)\n",
    "    golden = GrammarFuzzer(golden_grammar, start_symbol=start_symbol)\n",
    "    success = 0\n",
    "    while my_count != 0:\n",
    "        src = golden.fuzz()\n",
    "        try:\n",
    "            my_count -= 1\n",
    "            try:\n",
    "                # print('?', repr(src), file=sys.stderr)\n",
    "                for tree in ie.parse(src):\n",
    "                    success += 1\n",
    "                    break\n",
    "                if log: print(maximum - my_count, '+', repr(src), success, file=sys.stderr)\n",
    "            except:\n",
    "                #print(\"Error:\", sys.exc_info()[0], file=sys.stderr)\n",
    "                if log: print(maximum - my_count, '-', repr(src), file=sys.stderr)\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "    return (success, maximum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Precision\n",
    "How many of the inputs produced using our grammar are valid? (Accepted by the program)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:40.755549Z",
     "start_time": "2021-01-03T01:14:40.750692Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_precision(name, grammar, maximum=Max_Precision, start_symbol='<START>', log=False):\n",
    "    success = 0\n",
    "    with ExpectError():\n",
    "        fuzzer = GrammarFuzzer(grammar, start_symbol)\n",
    "        for i in range(maximum):\n",
    "            v = fuzzer.fuzz(key=start_symbol)\n",
    "            c = check_it(v, name)\n",
    "            success += (1 if c else 0)\n",
    "            if log: print(i, repr(v), c)\n",
    "    return (success, maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:40.763686Z",
     "start_time": "2021-01-03T01:14:40.759481Z"
    }
   },
   "outputs": [],
   "source": [
    "class timeit():\n",
    "    def __enter__(self):\n",
    "        self.tic = datetime.now()\n",
    "        return self\n",
    "    def __exit__(self, *args, **kwargs):\n",
    "        self.delta = datetime.now() - self.tic\n",
    "        self.runtime = (self.delta.microseconds, self.delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:41.678370Z",
     "start_time": "2021-01-03T01:14:41.675620Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from fuzzingbook.GrammarFuzzer import GrammarFuzzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:14:42.692993Z",
     "start_time": "2021-01-03T01:14:42.690114Z"
    }
   },
   "outputs": [],
   "source": [
    "BMiner_p = {}\n",
    "BMiner_r = {}\n",
    "\n",
    "BMiner_t ={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Golden Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:15:08.419363Z",
     "start_time": "2021-01-03T01:15:08.411217Z"
    }
   },
   "outputs": [],
   "source": [
    "calc_golden = {\n",
    "  \"<START>\": [\n",
    "    \"<expr>\"\n",
    "  ],\n",
    "  \"<expr>\": [\n",
    "    \"<term>+<expr>\",\n",
    "    \"<term>-<expr>\",\n",
    "    \"<term>\"\n",
    "  ],\n",
    "  \"<term>\": [\n",
    "    \"<factor>*<term>\",\n",
    "    \"<factor>/<term>\",\n",
    "    \"<factor>\"\n",
    "  ],\n",
    "  \"<factor>\": [\n",
    "    \"(<expr>)\",\n",
    "    \"<number>\"\n",
    "  ],\n",
    "  \"<number>\": [\n",
    "    \"<integer>.<integer>\",\n",
    "    \"<integer>\"\n",
    "  ],\n",
    "  \"<integer>\": [\n",
    "    \"<digit><integer>\",\n",
    "    \"<digit>\"\n",
    "  ],\n",
    "  \"<digit>\": [ \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\" ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:15:10.976304Z",
     "start_time": "2021-01-03T01:15:10.970703Z"
    }
   },
   "outputs": [],
   "source": [
    "calc_samples=[i.strip() for i in '''\\\n",
    "(1+2)*3/(423-334+9983)-5-((6)-(701))\n",
    "(123+133*(12-3)/9+8)+33\n",
    "(100)\n",
    "21*3\n",
    "33/44+2\n",
    "100\n",
    "23*234*22*4\n",
    "1+2\n",
    "31/20-2\n",
    "555+(234-445)\n",
    "1-(41/2)\n",
    "443-334+33-222\n",
    "'''.split('\\n') if i.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T01:15:50.461130Z",
     "start_time": "2021-01-03T01:15:16.109517Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1+2)*3/(423-334+9983)-5-((6)-(701))\n",
      "<main> ['<parse_expr-0-c>']\n",
      "<parse_expr-2> ['<parse_num-0-c>', '<parse_expr-3>']\n",
      "<parse_expr-5> ['<parse_expr-0-c>', '<parse_expr-6>']\n",
      "<parse_expr-8> ['<strlen-0-c>', '<parse_expr-9>']\n",
      "<__GI__dl_addr-1> ['<_dl_find_dso_for_object-1>', '<__tunable_get_val-0-c>']\n",
      "<parse_expr-6> ['<parse_expr-6-s>', '<parse_expr-0-c>']\n",
      "<parse_expr-3> ['<parse_expr-3-s>', '<parse_num-0-c>']\n",
      "<_int_malloc-1> ['0']\n",
      "<parse_num-2> ['<malloc-0-c>', '<__ctype_b_loc-0-c>']\n",
      "<parse_expr-11> ['<__ctype_b_loc-0-c>', '<parse_expr-12>']\n",
      "<strlen-10> ['7']\n",
      "<parse_expr-12> ['<parse_num-0-c>', '<parse_expr-13>']\n",
      "<__tunable_get_val-2> ['<parse_expr-0-c>', '<__tunable_get_val-3>']\n",
      "<strlen-9> ['6']\n",
      "<strlen-3> ['*']\n",
      "<strlen-8> ['5']\n",
      "<parse_expr-13> ['<strlen-0-c>', '<parse_expr-14>']\n",
      "<__tunable_get_val-3> ['<__tunable_get_val-3-s>', '<__tunable_get_val-4>']\n",
      "<__tunable_get_val-4> ['<parse_num-0-c>', '<__tunable_get_val-5>']\n",
      "<parse_expr-14> ['<__ctype_b_loc-0-c>', '<parse_num-0-c>']\n",
      "<__tunable_get_val-5> ['<__tunable_get_val-5-s>', '<__tunable_get_val-6>']\n",
      "<__ctype_b_loc-10> ['8']\n",
      "<__ctype_b_loc-3> ['+']\n",
      "<__tunable_get_val-6> ['<parse_expr-0-c>', '<__tunable_get_val-7>']\n",
      "<__ctype_b_loc-5> ['/']\n",
      "<__tunable_get_val-7> ['<__tunable_get_val-7-s>', '<__tunable_get_val-8>']\n",
      "<__tunable_get_val-8> ['<parse_num-0-c>', '<__tunable_get_val-9>']\n",
      "<__tunable_get_val-9> ['<__tunable_get_val-9-s>', '<__tunable_get_val-10>']\n",
      "<__tunable_get_val-10> ['<__ctype_b_loc-0-c>', '<parse_expr-0-c>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'<START>': ['<parse_expr-0-c>'],\n",
       " '<parse_expr-0-c>': ['<malloc-0-c>',\n",
       "  '<parse_expr-0-c><parse_expr-6-s><parse_expr-0-c>',\n",
       "  '<parse_num-0-c><parse_expr-3-s><parse_num-0-c>',\n",
       "  '<strlen-0-c><parse_expr-9>'],\n",
       " '<malloc-0-c>': ['<_dl_find_dso_for_object-1><__tunable_get_val-0-c>',\n",
       "  '<_int_malloc-0-c>'],\n",
       " '<parse_expr-6-s>': ['<strlen-0-c>', '<strlen-0-c><parse_expr-6-s>'],\n",
       " '<parse_num-0-c>': ['<malloc-0-c><__ctype_b_loc-0-c>', '<parse_num-1-s>'],\n",
       " '<parse_expr-3-s>': ['<strlen-0-c>', '<strlen-0-c><parse_expr-3-s>'],\n",
       " '<strlen-0-c>': ['*',\n",
       "  '5',\n",
       "  '6',\n",
       "  '7',\n",
       "  '<_dl_find_dso_for_object-1>',\n",
       "  '<strlen-11>',\n",
       "  '<strlen-2>',\n",
       "  '<strlen-4>',\n",
       "  '<strlen-5>',\n",
       "  '<strlen-6>',\n",
       "  '<strlen-7>'],\n",
       " '<parse_expr-9>': ['<parse_num-0-c>', '<parse_num-0-c><parse_expr-10>'],\n",
       " '<_dl_find_dso_for_object-1>': ['('],\n",
       " '<__tunable_get_val-0-c>': ['<__tunable_get_val-1>', '<_int_malloc-0-c>'],\n",
       " '<_int_malloc-0-c>': ['0', '<__ctype_b_loc-6>'],\n",
       " '<__tunable_get_val-1>': ['<__tunable_get_val-0-c>',\n",
       "  '<__tunable_get_val-0-c><parse_expr-0-c><__tunable_get_val-3-s><parse_num-0-c><__tunable_get_val-5-s><parse_expr-0-c><__tunable_get_val-7-s><parse_num-0-c><__tunable_get_val-9-s><__ctype_b_loc-0-c><parse_expr-0-c>'],\n",
       " '<__tunable_get_val-3-s>': ['<strlen-0-c>',\n",
       "  '<strlen-0-c><__tunable_get_val-3-s>'],\n",
       " '<__tunable_get_val-5-s>': ['<strlen-0-c>',\n",
       "  '<strlen-0-c><__tunable_get_val-5-s>'],\n",
       " '<__tunable_get_val-7-s>': ['<strlen-0-c>',\n",
       "  '<strlen-0-c><__tunable_get_val-7-s>'],\n",
       " '<__tunable_get_val-9-s>': ['<strlen-0-c>',\n",
       "  '<strlen-0-c><__tunable_get_val-9-s>'],\n",
       " '<__ctype_b_loc-0-c>': ['+',\n",
       "  '/',\n",
       "  '8',\n",
       "  '<__ctype_b_loc-6>',\n",
       "  '<_dl_find_dso_for_object-1>',\n",
       "  '<strlen-11>',\n",
       "  '<strlen-2>',\n",
       "  '<strlen-4>',\n",
       "  '<strlen-5>',\n",
       "  '<strlen-6>',\n",
       "  '<strlen-7>'],\n",
       " '<__ctype_b_loc-6>': ['1'],\n",
       " '<strlen-11>': ['9'],\n",
       " '<strlen-2>': [')'],\n",
       " '<strlen-4>': ['-'],\n",
       " '<strlen-5>': ['2'],\n",
       " '<strlen-6>': ['3'],\n",
       " '<strlen-7>': ['4'],\n",
       " '<parse_num-1-s>': ['<__ctype_b_loc-0-c>',\n",
       "  '<__ctype_b_loc-0-c><parse_num-1-s>'],\n",
       " '<parse_expr-10>': ['<strlen-0-c>',\n",
       "  '<strlen-0-c><__ctype_b_loc-0-c><parse_num-0-c><strlen-0-c><__ctype_b_loc-0-c><parse_num-0-c>']}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_grammar = binary_miner([calc_samples[0]], 'subject/calc/calc_parse.c')\n",
    "calc_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T17:03:55.867107Z",
     "start_time": "2021-01-02T17:03:55.850770Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(00*+*068580'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = GrammarFuzzer(calc_grammar, start_symbol='<START>')\n",
    "f.fuzz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-02T17:03:52.820Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'calculator' in CHECK:\n",
    "    result = check_recall(calc_golden, calc_grammar, start_symbol='<START>')\n",
    "    BMiner_r['calculator.py'] = result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T14:22:21.496167Z",
     "start_time": "2020-10-28T14:22:21.473384Z"
    }
   },
   "source": [
    "### Parson\n",
    "\n",
    "Parson is a lightweight json library written in C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-02T17:03:54.228Z"
    }
   },
   "outputs": [],
   "source": [
    "json_samples = [i.strip() for i in '''\\\n",
    "{\"emptya\":[],\"emptyh\":{},\"emptystr\":\"\",\"null\":\"null\"}\n",
    "{\"color\":\"blue\",\"category\":\"hue\",\"type\":\"primary\",\"code\":{\"rgba\":[0,0,255,1],\"hex\":\"#00F\"}}\n",
    "{\"color\":\"yellow\",\"category\":\"hue\",\"wetype\":\"primary\",\"code\":{\"rgba\":[255,255,0,1],\"hex\":\"#FF0\"}}\n",
    "{\"color\":\"green\",\"category\":\"hue\",\"type\":\"secondary\",\"code\":{\"rgba\":[0,255,0,1],\"hex\":\"#0F0\"}\n",
    "'''.split('\\n') if i.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-02T17:03:54.235Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#parson_grammar = binary_miner([json_samples[0]], 'subject/json/cJSON.c')\n",
    "#parson_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T11:31:53.698380Z",
     "start_time": "2021-01-01T11:31:52.881523Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MJS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Golden Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T17:05:44.469958Z",
     "start_time": "2021-01-02T17:05:44.463823Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "mjs_golden = {\n",
    "    '<START>': ['<program>'],\n",
    "    '<program>': ['<statement>'],\n",
    "    '<statement>': ['if <paren_expr> <statement>', 'if <paren_expr> <statement> else <statement>',\n",
    "                   'while <paren_expr> <statement>', 'do <statement> while <paren_expr>;',\n",
    "                   '{ <statement> }', '<expr> ;', ';'],\n",
    "    '<paren_expr>': ['(<expr>)'],\n",
    "    '<expr>': ['<test>', '<id>=<expr>'],\n",
    "    '<test>': ['<sum>', '<sum><<sum>'],\n",
    "    '<sum>': ['<term>', '<sum>+<term>', '<sum>-<term>'],\n",
    "    '<term>': ['<id>', '<int>', '<paren_expr>'],\n",
    "    '<id>': list(string.ascii_lowercase),\n",
    "    '<int>': list(string.digits)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T17:05:46.493216Z",
     "start_time": "2021-01-02T17:05:46.488813Z"
    }
   },
   "outputs": [],
   "source": [
    "mjs_samples=[i.strip() for i in '''\\\n",
    "{ i=7; if (i<5) x=1; if (i<10) y=2; }\n",
    "{ i=1; while (i<100) i=i+i; }\n",
    "a=b=c=2<3;\n",
    "{ i=1; while ((i=i+10)<50) ; }\n",
    "'''.split('\\n') if i.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T17:10:50.674970Z",
     "start_time": "2021-01-02T17:09:48.318955Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ i=7; if (i<5) x=1; if (i<10) y=2; }\n",
      "<main> ['<parse_statement_list-1>']\n",
      "<parse_if-1> ['<pnext-0-c>', '<parse_if-2>']\n",
      "<parse_comparison-1> ['<parse_shifts-1>', '<pnext-0-c>']\n",
      "<parse_if-2> ['<parse_assignment-0-c>', '<parse_if-3>']\n",
      "<parse_if-3> ['<pnext-0-c>', '<parse_block_or_stmt-1>']\n",
      "<findtok-1> ['0']\n",
      "<parse_block_or_stmt-1> ['<pnext-0-c>', '<parse_statement-0-c>']\n",
      "<longtok3-1> ['1']\n",
      "<longtok3-2> ['2']\n",
      "<*ABS*+0xa27b0-2> [';']\n",
      "<longtok3-3> ['5']\n",
      "<*ABS*+0xa27b0-1> [')']\n",
      "<skip_spaces_and_comments-3> ['{']\n",
      "<strlen-1> ['7']\n",
      "<mjs_is_space-4> ['x']\n",
      "<mjs_is_space-2> ['(']\n",
      "<mjs_is_space-5> ['y']\n",
      "<mjs_is_space-6> ['}']\n",
      "<mjs_is_alpha-1> ['<']\n",
      "<mjs_is_alpha-2> ['=']\n",
      "<mjs_is_alpha-3> ['f']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'<START>': ['<parse_statement_list-1>'],\n",
       " '<parse_statement_list-1>': ['<parse_statement_list-1-s>',\n",
       "  '<parse_statement_list-1-s><pnext-0-c>'],\n",
       " '<parse_statement_list-1-s>': ['<pnext-0-c><parse_statement-0-c>',\n",
       "  '<pnext-0-c><parse_statement-0-c><parse_statement_list-1-s>'],\n",
       " '<pnext-0-c>': ['<*ABS*+0xa27b0-0-c>', '<longtok3-0-c>', '<pnext-3>'],\n",
       " '<parse_statement-0-c>': ['<parse_assignment-0-c>',\n",
       "  '<parse_statement_list-1>',\n",
       "  '<pnext-0-c><parse_assignment-0-c><pnext-0-c><pnext-0-c><parse_statement-0-c>'],\n",
       " '<parse_assignment-0-c>': ['<parse_shifts-1><pnext-0-c>', '<pnext-0-c>'],\n",
       " '<parse_shifts-1>': ['<pnext-0-c>', '<pnext-0-c>0'],\n",
       " '<*ABS*+0xa27b0-0-c>': [')', ';'],\n",
       " '<longtok3-0-c>': ['1', '2', '5'],\n",
       " '<pnext-3>': ['<skip_spaces_and_comments-0-c>',\n",
       "  '<skip_spaces_and_comments-0-c><pnext-4>'],\n",
       " '<skip_spaces_and_comments-0-c>': ['<mjs_is_space-0-c>',\n",
       "  '<skip_spaces_and_comments-2>',\n",
       "  '{'],\n",
       " '<pnext-4>': ['<mjs_is_ident-0-c>', '<mjs_is_ident-0-c>7'],\n",
       " '<mjs_is_space-0-c>': ['(',\n",
       "  '<mjs_is_space-1>',\n",
       "  '<skip_spaces_and_comments-2>',\n",
       "  'x',\n",
       "  'y',\n",
       "  '}'],\n",
       " '<skip_spaces_and_comments-2>': ['i'],\n",
       " '<mjs_is_space-1>': [' '],\n",
       " '<mjs_is_ident-0-c>': ['<mjs_is_alpha-0-c>', '<mjs_is_space-1>'],\n",
       " '<mjs_is_alpha-0-c>': ['<', '=', 'f']}"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mjs_grammar = binary_miner(mjs_samples, 'subject/mjs/mjs.c')\n",
    "mjs_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-02T17:03:56.376Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'mjs' in CHECK:\n",
    "    result = check_recall(mjs_golden, mjs_grammar, start_symbol='<START>')\n",
    "    BMiner_r['mjs.c'] = result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T17:11:07.457734Z",
     "start_time": "2021-01-02T17:11:07.431462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{;5))21;;55;5;'"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = GrammarFuzzer(mjs_grammar, start_symbol='<START>')\n",
    "f.fuzz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duktape\n",
    "Duktape is an embeddable Javascript engine, with a focus on portability and compact footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-02T17:04:04.807Z"
    }
   },
   "outputs": [],
   "source": [
    "duktape_samples=[i.strip() for i in '''\\\n",
    "\"a = 'Hello world!';\"\n",
    "'''.split('\\n') if i.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-02T17:04:04.812Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "duktape_grammar = binary_miner(duktape_samples, 'subject/duktape/duktape.c')\n",
    "duktape_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Golden Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T17:11:19.949812Z",
     "start_time": "2021-01-02T17:11:19.939604Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "CSV_GRAMMAR = {\n",
    "    '<START>': ['<csvline>'],\n",
    "    '<csvline>': ['<items>'],\n",
    "    '<items>': ['<item>,<items>', '<item>'],\n",
    "    '<item>': ['<letters>'],\n",
    "    '<letters>': ['<letter><letters>', '<letter>'],\n",
    "    '<letter>': list(string.ascii_letters + string.digits + string.punctuation + ' \\t\\n')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T17:11:22.265532Z",
     "start_time": "2021-01-02T17:11:22.251837Z"
    }
   },
   "outputs": [],
   "source": [
    "from fuzzingbook.GrammarMiner import VEHICLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T17:12:44.030067Z",
     "start_time": "2021-01-02T17:11:27.975318Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1997,van,Ford,E350\n",
      "2000,car,Mercury,Cougar\n",
      "1999,car,Chevy,Venture\n",
      "<main> ['<__GI__dl_addr-1>']\n",
      "<__GI__dl_addr-1> ['<_dl_find_dso_for_object-0-c>', '<__tunable_get_val-0-c>']\n",
      "<_dl_find_dso_for_object-1> ['1']\n",
      "<_dl_find_dso_for_object-2> ['2']\n",
      "<csv_load-1> ['<append_row-1>', '<csv_load-2-s>']\n",
      "<append_row-1> ['<realloc-0-c>', '<strcpy-1>']\n",
      "<read_next_field-2> ['<add_char-1>', '<read_next_field-3-s>']\n",
      "<getc-14> ['d']\n",
      "<getc-22> ['u']\n",
      "<getc-11> ['V']\n",
      "<getc-4> ['5']\n",
      "<getc-5> ['7']\n",
      "<getc-19> ['o']\n",
      "<getc-9> ['F']\n",
      "<getc-18> ['n']\n",
      "<getc-20> ['r']\n",
      "<getc-3> ['3']\n",
      "<getc-15> ['e']\n",
      "<getc-8> ['E']\n",
      "<getc-21> ['t']\n",
      "<getc-10> ['M']\n",
      "<getc-12> ['a']\n",
      "<getc-23> ['v']\n",
      "<getc-16> ['g']\n",
      "<getc-17> ['h']\n",
      "<getc-13> ['c']\n",
      "<getc-24> ['y']\n",
      "<getc-7> ['C']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'<START>': ['<_dl_find_dso_for_object-0-c><__tunable_get_val-0-c>'],\n",
       " '<_dl_find_dso_for_object-0-c>': ['1', '2'],\n",
       " '<__tunable_get_val-0-c>': ['<__tunable_get_val-1>',\n",
       "  '<_int_malloc-1>',\n",
       "  '<csv_load-0-c>'],\n",
       " '<__tunable_get_val-1>': ['<__tunable_get_val-0-c>',\n",
       "  '<__tunable_get_val-0-c><csv_load-0-c>'],\n",
       " '<_int_malloc-1>': ['0'],\n",
       " '<csv_load-0-c>': ['<read_next_field-1>',\n",
       "  '<realloc-0-c><strcpy-1><csv_load-2-s>'],\n",
       " '<read_next_field-1>': ['<read_next_field-1-s>',\n",
       "  '<read_next_field-1-s><add_char-1><read_next_field-3-s>'],\n",
       " '<realloc-0-c>': ['<_int_malloc-1>', '<_int_realloc-1>'],\n",
       " '<strcpy-1>': [','],\n",
       " '<csv_load-2-s>': ['<read_next_field-1>',\n",
       "  '<read_next_field-1><csv_load-2-s>'],\n",
       " '<read_next_field-1-s>': ['<getc-0-c>', '<getc-0-c><read_next_field-1-s>'],\n",
       " '<add_char-1>': ['9', '9<realloc-0-c>'],\n",
       " '<read_next_field-3-s>': ['<getc-0-c>', '<getc-0-c><read_next_field-3-s>'],\n",
       " '<getc-0-c>': ['3',\n",
       "  '5',\n",
       "  '7',\n",
       "  '<_int_malloc-1>',\n",
       "  '<_int_realloc-1>',\n",
       "  '<strcpy-1>',\n",
       "  'C',\n",
       "  'E',\n",
       "  'F',\n",
       "  'M',\n",
       "  'V',\n",
       "  'a',\n",
       "  'c',\n",
       "  'd',\n",
       "  'e',\n",
       "  'g',\n",
       "  'h',\n",
       "  'n',\n",
       "  'o',\n",
       "  'r',\n",
       "  't',\n",
       "  'u',\n",
       "  'v',\n",
       "  'y'],\n",
       " '<_int_realloc-1>': ['9']}"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_grammar = binary_miner(VEHICLES, 'subject/csv/csv.c')\n",
    "csv_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-02T17:03:58.217Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'csv' in CHECK:\n",
    "    result = check_recall(CSV_GRAMMAR, csv_grammar, start_symbol='<START>')\n",
    "    BMiner_r['csv.c'] = result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T17:13:00.190271Z",
     "start_time": "2021-01-02T17:13:00.182627Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2t970,oh'"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = GrammarFuzzer(csv_grammar, start_symbol='<START>')\n",
    "f.fuzz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TinyC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Golden Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T17:13:04.511458Z",
     "start_time": "2021-01-02T17:13:04.501903Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "tinyc_golden = {\n",
    "    '<START>': ['<program>'],\n",
    "    '<program>': ['<statement>'],\n",
    "    '<statement>': ['if <paren_expr> <statement>', 'if <paren_expr> <statement> else <statement>',\n",
    "                   'while <paren_expr> <statement>', 'do <statement> while <paren_expr>;',\n",
    "                   '{ <statement> }', '<expr> ;', ';'],\n",
    "    '<paren_expr>': ['(<expr>)'],\n",
    "    '<expr>': ['<test>', '<id>=<expr>'],\n",
    "    '<test>': ['<sum>', '<sum><<sum>'],\n",
    "    '<sum>': ['<term>', '<sum>+<term>', '<sum>-<term>'],\n",
    "    '<term>': ['<id>', '<int>', '<paren_expr>'],\n",
    "    '<id>': list(string.ascii_lowercase),\n",
    "    '<int>': list(string.digits)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T17:13:07.916856Z",
     "start_time": "2021-01-02T17:13:07.911475Z"
    }
   },
   "outputs": [],
   "source": [
    "tiny_samples=[i.strip() for i in '''\\\n",
    "{ i=1; while (i<100) i=i+i; }\n",
    "a=b=c=2<3;\n",
    "{ i=125; j=100; while (i-j) if (i<j) j=j-i; else i=i-j; }\n",
    "{ i=1; do i=i+10; while (i<50); }\n",
    "{ i=1; while ((i=i+10)<50) ; }\n",
    "{ i=7; if (i<5) x=1; if (i<10) y=2; }\n",
    "'''.split('\\n') if i.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T17:13:39.657246Z",
     "start_time": "2021-01-02T17:13:14.360863Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ i=1; while (i<100) i=i+i; }\n",
      "<main> ['<new_node-0-c>']\n",
      "<new_node-1> ['(']\n",
      "<__GI__dl_addr-1> ['<_dl_find_dso_for_object-1>', '<__tunable_get_val-0-c>']\n",
      "<_dl_find_dso_for_object-1> ['{']\n",
      "<_int_malloc-2> ['1']\n",
      "<__tunable_get_val-2> ['<next_sym-1>', '<statement-0-c>']\n",
      "<statement-3> ['<next_sym-1>', '<statement-3-c>']\n",
      "<next_ch-11> ['l']\n",
      "<next_ch-6> ['<']\n",
      "<next_ch-2> [')']\n",
      "<next_ch-12> ['w']\n",
      "<next_ch-5> [';']\n",
      "<next_ch-8> ['e']\n",
      "<next_ch-3> ['+']\n",
      "<statement-4> ['<new_node-0-c>', '<statement-5>']\n",
      "<next_ch-7> ['=']\n",
      "<next_ch-9> ['h']\n",
      "<next_ch-1> [' ']\n",
      "<statement-8> ['<paren_expr-1>', '<statement-0-c>']\n",
      "<next_ch-13> ['}']\n",
      "<paren_expr-1> ['<next_sym-1>', '<statement-1>']\n",
      "<statement-5> ['<statement-0-c>', '<statement-6>']\n",
      "<statement-6> ['<new_node-0-c>', '<statement-0-c>']\n",
      "<expr-1> ['<next_sym-1>', '<expr-0-c>']\n",
      "<test-2> ['<new_node-0-c>', '<test-3>']\n",
      "<test-3> ['<next_sym-1>', '<sum-0-c>']\n",
      "<sum-1> ['<next_sym-1>', '<term-0-c>']\n",
      "<__tunable_get_val-0-c> ['<next_sym-1>', '<statement-0-c>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'<START>': ['<new_node-0-c>'],\n",
       " '<new_node-0-c>': ['(', '<malloc-0-c>'],\n",
       " '<malloc-0-c>': ['<_int_malloc-0-c>', '{<next_sym-1><statement-0-c>'],\n",
       " '<_int_malloc-0-c>': ['1', '<next_ch-4>'],\n",
       " '<next_sym-1>': ['<next_sym-1-s>', '<next_sym-1-s><strcmp-1>'],\n",
       " '<statement-0-c>': ['<next_sym-1><statement-3-c>', '<statement-1>'],\n",
       " '<next_ch-4>': ['0'],\n",
       " '<next_sym-1-s>': ['<next_ch-0-c>', '<next_ch-0-c><next_sym-1-s>'],\n",
       " '<strcmp-1>': ['i'],\n",
       " '<next_ch-0-c>': [' ',\n",
       "  ')',\n",
       "  '+',\n",
       "  ';',\n",
       "  '<',\n",
       "  '<next_ch-4>',\n",
       "  '<strcmp-1>',\n",
       "  '=',\n",
       "  'e',\n",
       "  'h',\n",
       "  'l',\n",
       "  'w',\n",
       "  '}'],\n",
       " '<statement-3-c>': ['<new_node-0-c><statement-0-c><new_node-0-c><statement-0-c>',\n",
       "  '<next_sym-1><statement-1><statement-0-c>'],\n",
       " '<statement-1>': ['<expr-0-c><next_sym-1>'],\n",
       " '<expr-0-c>': ['<next_sym-1><expr-0-c>', '<test-1>'],\n",
       " '<test-1>': ['<sum-0-c>', '<sum-0-c><new_node-0-c><next_sym-1><sum-0-c>'],\n",
       " '<sum-0-c>': ['<next_sym-1><term-0-c>', '<term-0-c>'],\n",
       " '<term-0-c>': ['<new_node-0-c>', '<next_sym-1>']}"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_grammar = binary_miner([tiny_samples[0]], 'subject/tinyC/tiny.c')\n",
    "tiny_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-02T17:03:59.855Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'tinyc' in CHECK:\n",
    "    result = check_recall(calc_golden, tiny_grammar, start_symbol='<START>')\n",
    "    BMiner_r['tiny.c'] = result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T17:13:56.884511Z",
     "start_time": "2021-01-02T17:13:56.871086Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{<e)}ll1(; lhi'"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = GrammarFuzzer(tiny_grammar, start_symbol='<START>')\n",
    "f.fuzz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Golden Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T17:14:05.352231Z",
     "start_time": "2021-01-02T17:14:05.349524Z"
    }
   },
   "outputs": [],
   "source": [
    "from fuzzingbook.Grammars import srange\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T17:14:06.700658Z",
     "start_time": "2021-01-02T17:14:06.690208Z"
    }
   },
   "outputs": [],
   "source": [
    "XML_GRAMMAR = {\n",
    "    \"<START>\": [\"<xml-tree>\"],\n",
    "    \"<xml-tree>\": [\"<text>\",\n",
    "                   \"<xml-open-tag><xml-tree><xml-close-tag>\", \n",
    "                   \"<xml-openclose-tag>\", \n",
    "                   \"<xml-tree><xml-tree>\"],\n",
    "    \"<xml-open-tag>\":      [\"<<id>>\", \"<<id> <xml-attribute>>\"],\n",
    "    \"<xml-openclose-tag>\": [\"<<id>/>\", \"<<id> <xml-attribute>/>\"],\n",
    "    \"<xml-close-tag>\":     [\"</<id>>\"],\n",
    "    \"<xml-attribute>\" :    [\"<id>=<id>\", \"<xml-attribute> <xml-attribute>\"],\n",
    "    \"<id>\":                [\"<letter>\", \"<id><letter>\"],\n",
    "    \"<text>\" :             [\"<text><letter_space>\",\"<letter_space>\"],\n",
    "    \"<letter>\":            srange(string.ascii_letters + string.digits +\"\\\"\"+\"'\"+\".\"),\n",
    "    \"<letter_space>\":      srange(string.ascii_letters + string.digits +\"\\\"\"+\"'\"+\" \"+\"\\t\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T17:14:09.325281Z",
     "start_time": "2021-01-02T17:14:09.321249Z"
    }
   },
   "outputs": [],
   "source": [
    "xml_samples=[i.strip() for i in '''\\\n",
    "<note></note>\n",
    "<to>Tove</to>\n",
    "<from>Jani</from>\n",
    "<heading>Reminder</heading>\n",
    "'''.split('\\n') if i.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T17:14:33.409370Z",
     "start_time": "2021-01-02T17:14:11.478834Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<note></note>\n",
      "<main> ['<__GI__dl_addr-1>']\n",
      "<__GI__dl_addr-1> ['<_dl_find_dso_for_object-1>', '<__tunable_get_val-0-c>']\n",
      "<__tunable_get_val-3> ['<_int_malloc-0-c>', '<_int_malloc-0-c>']\n",
      "<yxml_parse-1> ['/']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'<START>': ['<_dl_find_dso_for_object-1><__tunable_get_val-0-c>'],\n",
       " '<_dl_find_dso_for_object-1>': ['<'],\n",
       " '<__tunable_get_val-0-c>': ['<__tunable_get_val-1>',\n",
       "  '<_int_malloc-0-c><_int_malloc-0-c>'],\n",
       " '<__tunable_get_val-1>': ['<__tunable_get_val-0-c>',\n",
       "  '<__tunable_get_val-0-c><__tunable_get_val-2-s>'],\n",
       " '<_int_malloc-0-c>': ['<yxml_elemclose-3>', '<yxml_parse-5>'],\n",
       " '<__tunable_get_val-2-s>': ['<yxml_parse-0-c>',\n",
       "  '<yxml_parse-0-c><__tunable_get_val-2-s>'],\n",
       " '<yxml_parse-0-c>': ['/',\n",
       "  '<_dl_find_dso_for_object-1>',\n",
       "  '<yxml_elemclose-0-c>',\n",
       "  '<yxml_parse-5>',\n",
       "  '<yxml_pushstackc-0-c>'],\n",
       " '<yxml_elemclose-0-c>': ['<yxml_elemclose-1>',\n",
       "  '<yxml_elemclose-2>',\n",
       "  '<yxml_elemclose-3>',\n",
       "  '<yxml_elemclose-4>'],\n",
       " '<yxml_parse-5>': ['n'],\n",
       " '<yxml_pushstackc-0-c>': ['<yxml_elemclose-1>',\n",
       "  '<yxml_elemclose-2>',\n",
       "  '<yxml_elemclose-4>'],\n",
       " '<yxml_elemclose-1>': ['>'],\n",
       " '<yxml_elemclose-2>': ['e'],\n",
       " '<yxml_elemclose-3>': ['o'],\n",
       " '<yxml_elemclose-4>': ['t']}"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yxml_grammar = binary_miner([xml_samples[0]], 'subject/xml/yxml.c')\n",
    "yxml_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-02T17:04:01.893Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'yxml' in CHECK:\n",
    "    result = check_recall(XML_GRAMMAR, yxml_grammar, start_symbol='<START>')\n",
    "    BMiner_r['yxml.c'] = result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T17:14:52.618660Z",
     "start_time": "2021-01-02T17:14:52.528674Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<oo\n",
      "<no\n",
      "<non/>\n",
      "<oo<<\n",
      "<no////>><////\n",
      "<nn\n",
      "<on\n",
      "<no\n",
      "<oo\n",
      "<nne\n",
      "<on\n",
      "<onn/</ont\n",
      "<no\n",
      "<on\n",
      "<non\n",
      "<no\n",
      "<nn\n",
      "<no\n",
      "<no\n",
      "<not/ten<<\n",
      "<nn\n",
      "<on\n",
      "<no\n",
      "<no\n",
      "<oo\n",
      "<no\n",
      "<oo/\n",
      "<on\n",
      "<on\n",
      "<one/e/\n",
      "<on<nt\n",
      "<no<n\n",
      "<nnn\n",
      "<on\n",
      "<no\n",
      "<on\n",
      "<no\n",
      "<nnt<\n",
      "<oo>\n",
      "<nn/et>\n",
      "<on\n",
      "<on\n",
      "<oo<t/n\n",
      "<on\n",
      "<oon\n",
      "<nn\n",
      "<oo\n",
      "<onent\n",
      "<oo\n",
      "<oo\n",
      "<ootoe\n",
      "<oo\n",
      "<oo\n",
      "<onon\n",
      "<no\n",
      "<oo\n",
      "<onn\n",
      "<oo\n",
      "<nn\n",
      "<oo\n",
      "<no\n",
      "<on\n",
      "<nn\n",
      "<on\n",
      "<no\n",
      "<nn\n",
      "<no\n",
      "<oo<<<\n",
      "<on/n\n",
      "<on\n",
      "<oo\n",
      "<no\n",
      "<oo>tnt\n",
      "<no\n",
      "<onnnt<\n",
      "<on\n",
      "<no\n",
      "<nn/\n",
      "<oo\n",
      "<oneon\n",
      "<oot<en/>/<\n",
      "<non\n",
      "<no\n",
      "<oo\n",
      "<nn\n",
      "<on/<\n",
      "<nn>\n",
      "<nnn\n",
      "<nn\n",
      "<oo\n",
      "<noet>e/\n",
      "<no\n",
      "<oo\n",
      "<on\n",
      "<on/>\n",
      "<no\n",
      "<on\n",
      "<on\n",
      "<no\n",
      "<oo/\n"
     ]
    }
   ],
   "source": [
    "f = GrammarFuzzer(yxml_grammar, start_symbol='<START>')\n",
    "for i in range(100):\n",
    "    print(f.fuzz())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "801px",
    "left": "23px",
    "top": "110px",
    "width": "436.354px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
